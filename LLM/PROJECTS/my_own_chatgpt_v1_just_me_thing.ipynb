{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install pymupdf\n",
        "!pip install transformers\n",
        "!pip install peft\n",
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqJvQEnN6vHB",
        "outputId": "c50a5cb0-3c75-40eb-b722-453018ce8b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.5.1+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.48.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.3.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2024.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.2)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NL2ZFObs3eH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import fitz\n",
        "import torch.nn as nn\n",
        "import re\n",
        "import torch.optim as option\n",
        "from collections import Counter\n",
        "from transformers import BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, TaskType"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Tokenizer"
      ],
      "metadata": {
        "id": "W6acSLou92vH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTokenizer:\n",
        "    def __init__(self, vocab_size=8000):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.vocab = {\"<pad>\": 0, \"<unk>\": 1, \"<bos>\": 2, \"<eos>\": 3}\n",
        "        self.rev_vocab = None\n",
        "\n",
        "    def build_vocab(self, text):\n",
        "        tokens = re.findall(r'\\w+|[.,!?;]', text.lower())\n",
        "        token_counts = Counter(tokens)\n",
        "        most_common = token_counts.most_common(self.vocab_size - 4)\n",
        "        self.vocab.update({token: idx + 4 for idx, (token, _) in enumerate(most_common)})\n",
        "        self.rev_vocab = {v: k for k, v in self.vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        tokens = re.findall(r'\\w+|[.,!?;]', text.lower())\n",
        "        return [self.vocab.get(token, self.vocab[\"<unk>\"]) for token in tokens]\n",
        "\n",
        "    def decode(self, indices):\n",
        "        return ' '.join([self.rev_vocab.get(idx, \"<unk>\") for idx in indices])"
      ],
      "metadata": {
        "id": "VJvNlsmW97Hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data and process"
      ],
      "metadata": {
        "id": "Vigg8pZWq0Dv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pdf_text(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \" \".join(page.get_text(\"text\") for page in doc)\n",
        "    return text"
      ],
      "metadata": {
        "id": "Dxs2euZBqtfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####for both small and big data\n",
        "\n",
        "# class RelativePositionalEncoding(nn.Module):\n",
        "#     def __init__(self, d_model, max_len):\n",
        "#         super().__init__()\n",
        "#         self.relative_positions = nn.Parameter(torch.randn(max_len, d_model))\n",
        "\n",
        "#     def forward(self, positions):\n",
        "#         return self.relative_positions[positions]\n",
        "\n",
        "# rpe = RelativePositionalEncoding(d_model=256, max_len=100)\n",
        "# positions = torch.arange(0, 10)\n",
        "# encoded_positions = rpe(positions)\n",
        "# print(encoded_positions.shape)"
      ],
      "metadata": {
        "id": "hacJy56Bqtce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### for medium type data\n",
        "\n",
        "# def rotary_embedding(x, theta=10000):\n",
        "#     seq_len, dim = x.shape\n",
        "#     freqs = torch.pow(theta, -torch.arange(0, dim, 2) / dim)\n",
        "#     angles = torch.arange(seq_len).unsqueeze(1) * freqs.unsqueeze(0)\n",
        "#     x_rotated = torch.cat([x[:, 0::2] * torch.cos(angles) - x[:, 1::2] * torch.sin(angles),\n",
        "#                            x[:, 0::2] * torch.sin(angles) + x[:, 1::2] * torch.cos(angles)], dim=-1)\n",
        "#     return x_rotated\n",
        "\n",
        "# x = torch.randn(10, 512)\n",
        "# x_rope = rotary_embedding(x)\n",
        "# print(x_rope.shape)"
      ],
      "metadata": {
        "id": "m7LHPfqhqtaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### for small data\n",
        "\n",
        "class LearnedPositionalEncoding(nn.Module):\n",
        "    def __init__(self, max_len, d_model):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(max_len, d_model)\n",
        "\n",
        "    def forward(self, positions):\n",
        "        return self.embedding(positions.long())"
      ],
      "metadata": {
        "id": "exok0BYwqtXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "M6OjbAFrELcK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rnn is good gor real time and small  data processing\n",
        "\n",
        "Rest is Casulal Transformer"
      ],
      "metadata": {
        "id": "eddeWZH5XMZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN-based Model (Using GRU Instead of Transformer)"
      ],
      "metadata": {
        "id": "f3hgSFoVWaHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class RNNLanguageModel(nn.Module):\n",
        "#     def __init__(self, vocab_size, embed_dim=256, hidden_dim=512, num_layers=2):\n",
        "#         super().__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "#         self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "#         self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.embedding(x)\n",
        "#         x, _ = self.gru(x)  # Using GRU\n",
        "#\n",
        "#         return self.fc(x)"
      ],
      "metadata": {
        "id": "HJtkycMEWWvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN-based Model (Using LSTM Instead of Transformer)"
      ],
      "metadata": {
        "id": "IDtULbWpWpeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class RNNLanguageModel(nn.Module):\n",
        "#     def __init__(self, vocab_size, embed_dim=256, hidden_dim=512, num_layers=2):\n",
        "#         super().__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "#         self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "#         self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.embedding(x)\n",
        "#         x, _ = self.lstm(x)\n",
        "\n",
        "#         return self.fc(x)"
      ],
      "metadata": {
        "id": "-WOJmou5WvZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Casual Transformer"
      ],
      "metadata": {
        "id": "GBBiJvb_kZju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=256, num_heads=8, hidden_dim=512, num_layers=6, max_len=1000):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.pos_encoder = LearnedPositionalEncoding(max_len, embed_dim)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
        "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n",
        "        x = self.embedding(x)\n",
        "        x = x + self.pos_encoder(positions)\n",
        "        attn_mask = torch.triu(torch.ones(x.size(1), x.size(1), device=x.device), diagonal=1).bool()\n",
        "        return self.lm_head(self.transformer(x, mask=attn_mask))"
      ],
      "metadata": {
        "id": "P14RFxYLqtVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# appling Lora"
      ],
      "metadata": {
        "id": "KclIpkzGbW4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_lora(model):\n",
        "    lora_config = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.05,\n",
        "        target_modules=[\"lm_head\"]\n",
        "    )\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    return model"
      ],
      "metadata": {
        "id": "l_Ae4FYOqtOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# appling QLorra"
      ],
      "metadata": {
        "id": "FFHwpvzcg7h1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_qlora(model):\n",
        "    q_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True, bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16\n",
        "    )\n",
        "    model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    return get_peft_model(model, q_config)"
      ],
      "metadata": {
        "id": "eGoxUsoGewGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Function with Accuracy"
      ],
      "metadata": {
        "id": "EzJKxhq3CTVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataset, epochs=500, batch_size=4, lr=3e-5, max_len=100):\n",
        "    model.train()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        inputs = dataset[:, :-1]\n",
        "        targets = dataset[:, 1:]\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs.reshape(-1, outputs.size(-1)), targets.reshape(-1))\n",
        "\n",
        "        predictions = torch.argmax(outputs, dim=-1)\n",
        "        correct = (predictions == targets).float()\n",
        "        mask = (targets != 0).float()\n",
        "        accuracy = (correct * mask).sum() / mask.sum()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Perplexity\n",
        "        perplexity = math.exp(loss.item()) if loss.item() < 300 else float('inf')\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Perplexity: {perplexity:.4f}, Accuracy: {accuracy.item() * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "cS6L5ROnCXHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Response"
      ],
      "metadata": {
        "id": "M0_tG38nITpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(model, tokenizer, prompt, max_len=50, temperature=0.8, top_k=10):\n",
        "    model.eval()\n",
        "    tokens = tokenizer.encode(prompt) + [tokenizer.vocab[\"<bos>\"]]\n",
        "    input_tensor = torch.tensor(tokens).unsqueeze(0)\n",
        "    response = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len):\n",
        "            output = model(input_tensor)\n",
        "            probs = torch.nn.functional.softmax(output[:, -1, :] / temperature, dim=-1)\n",
        "            top_k_probs, top_k_indices = torch.topk(probs, top_k)\n",
        "            next_token = top_k_indices[0, torch.multinomial(top_k_probs, num_samples=1)].item()\n",
        "            if next_token == tokenizer.vocab[\"<eos>\"]:\n",
        "                break\n",
        "            response.append(next_token)\n",
        "            input_tensor = torch.cat([input_tensor, torch.tensor([[next_token]])], dim=1)\n",
        "    return tokenizer.decode(response)"
      ],
      "metadata": {
        "id": "Ycil3Nd3l70d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/drive/MyDrive/Bnagla.txt\"\n",
        "pdf_text = load_pdf_text(pdf_path)\n",
        "tokenizer = CustomTokenizer()\n",
        "tokenizer.build_vocab(pdf_text)\n",
        "\n",
        "vocab_size = len(tokenizer.vocab)\n",
        "embed_dim = 256\n",
        "num_heads = 8\n",
        "hidden_dim = 512\n",
        "num_layers = 6\n",
        "max_len = 100"
      ],
      "metadata": {
        "id": "LJXydzUpVN7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply LoRA or QLoRA"
      ],
      "metadata": {
        "id": "kI7YAZQjVaao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = CausalTransformer(vocab_size, embed_dim, num_heads, hidden_dim, num_layers, max_len)\n",
        "\n",
        "# use_qlora = True\n",
        "# if use_qlora:\n",
        "#     model = apply_qlora(model)\n",
        "# else:\n",
        "#     model = apply_lora(model)\n",
        "\n",
        "# dataset = torch.tensor([tokenizer.encode(pdf_text)[:max_len] + [0] * (max_len - len(tokenizer.encode(pdf_text)[:max_len]))], dtype=torch.long)\n",
        "\n",
        "# train_model(model, dataset)\n",
        "\n",
        "# while True:\n",
        "#     user_input = input(\"You: \")\n",
        "#     if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "#         break\n",
        "#     response = generate_response(model, tokenizer, user_input)\n",
        "#     print(\"Bot:\", response)"
      ],
      "metadata": {
        "id": "3-VNtQhKVVXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CausalTransformer(vocab_size, embed_dim, num_heads, hidden_dim, num_layers, max_len)\n",
        "dataset = torch.tensor([tokenizer.encode(pdf_text)[:max_len] + [0] * (max_len - len(tokenizer.encode(pdf_text)[:max_len]))], dtype=torch.long)\n",
        "\n",
        "train_model(model, dataset)\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "        break\n",
        "    response = generate_response(model, tokenizer, user_input)\n",
        "    print(\"Bot:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42uTlVTiiEGP",
        "outputId": "5abe107d-fb8c-4414-9f8b-495608f7e6e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500, Loss: 6.9726, Perplexity: 1067.0274, Accuracy: 0.00%\n",
            "Epoch 2/500, Loss: 6.8762, Perplexity: 968.9681, Accuracy: 0.00%\n",
            "Epoch 3/500, Loss: 6.8447, Perplexity: 938.9026, Accuracy: 0.00%\n",
            "Epoch 4/500, Loss: 6.7766, Perplexity: 877.0911, Accuracy: 0.00%\n",
            "Epoch 5/500, Loss: 6.6941, Perplexity: 807.6463, Accuracy: 0.00%\n",
            "Epoch 6/500, Loss: 6.6331, Perplexity: 759.8420, Accuracy: 3.03%\n",
            "Epoch 7/500, Loss: 6.5684, Perplexity: 712.2239, Accuracy: 3.03%\n",
            "Epoch 8/500, Loss: 6.5222, Perplexity: 680.0444, Accuracy: 3.03%\n",
            "Epoch 9/500, Loss: 6.4527, Perplexity: 634.4254, Accuracy: 6.06%\n",
            "Epoch 10/500, Loss: 6.4081, Perplexity: 606.7500, Accuracy: 8.08%\n",
            "Epoch 11/500, Loss: 6.3421, Perplexity: 567.9719, Accuracy: 11.11%\n",
            "Epoch 12/500, Loss: 6.3027, Perplexity: 546.0463, Accuracy: 11.11%\n",
            "Epoch 13/500, Loss: 6.2508, Perplexity: 518.4042, Accuracy: 12.12%\n",
            "Epoch 14/500, Loss: 6.2085, Perplexity: 496.9784, Accuracy: 12.12%\n",
            "Epoch 15/500, Loss: 6.1483, Perplexity: 467.9385, Accuracy: 12.12%\n",
            "Epoch 16/500, Loss: 6.1130, Perplexity: 451.6818, Accuracy: 12.12%\n",
            "Epoch 17/500, Loss: 6.0745, Perplexity: 434.6367, Accuracy: 12.12%\n",
            "Epoch 18/500, Loss: 6.0247, Perplexity: 413.5275, Accuracy: 12.12%\n",
            "Epoch 19/500, Loss: 5.9834, Perplexity: 396.8028, Accuracy: 12.12%\n",
            "Epoch 20/500, Loss: 5.9638, Perplexity: 389.0762, Accuracy: 12.12%\n",
            "Epoch 21/500, Loss: 5.9100, Perplexity: 368.7010, Accuracy: 12.12%\n",
            "Epoch 22/500, Loss: 5.8908, Perplexity: 361.7049, Accuracy: 12.12%\n",
            "Epoch 23/500, Loss: 5.8510, Perplexity: 347.5786, Accuracy: 12.12%\n",
            "Epoch 24/500, Loss: 5.8199, Perplexity: 336.9246, Accuracy: 12.12%\n",
            "Epoch 25/500, Loss: 5.7943, Perplexity: 328.4283, Accuracy: 12.12%\n",
            "Epoch 26/500, Loss: 5.7699, Perplexity: 320.5153, Accuracy: 12.12%\n",
            "Epoch 27/500, Loss: 5.7351, Perplexity: 309.5452, Accuracy: 12.12%\n",
            "Epoch 28/500, Loss: 5.7040, Perplexity: 300.0687, Accuracy: 12.12%\n",
            "Epoch 29/500, Loss: 5.6753, Perplexity: 291.5875, Accuracy: 12.12%\n",
            "Epoch 30/500, Loss: 5.6256, Perplexity: 277.4336, Accuracy: 12.12%\n",
            "Epoch 31/500, Loss: 5.6113, Perplexity: 273.4943, Accuracy: 12.12%\n",
            "Epoch 32/500, Loss: 5.5999, Perplexity: 270.3868, Accuracy: 12.12%\n",
            "Epoch 33/500, Loss: 5.5840, Perplexity: 266.1441, Accuracy: 12.12%\n",
            "Epoch 34/500, Loss: 5.5616, Perplexity: 260.2468, Accuracy: 12.12%\n",
            "Epoch 35/500, Loss: 5.5178, Perplexity: 249.0804, Accuracy: 12.12%\n",
            "Epoch 36/500, Loss: 5.4743, Perplexity: 238.4730, Accuracy: 14.14%\n",
            "Epoch 37/500, Loss: 5.4628, Perplexity: 235.7679, Accuracy: 12.12%\n",
            "Epoch 38/500, Loss: 5.4283, Perplexity: 227.7537, Accuracy: 13.13%\n",
            "Epoch 39/500, Loss: 5.4065, Perplexity: 222.8505, Accuracy: 13.13%\n",
            "Epoch 40/500, Loss: 5.4055, Perplexity: 222.6375, Accuracy: 13.13%\n",
            "Epoch 41/500, Loss: 5.3722, Perplexity: 215.3370, Accuracy: 12.12%\n",
            "Epoch 42/500, Loss: 5.3640, Perplexity: 213.5691, Accuracy: 13.13%\n",
            "Epoch 43/500, Loss: 5.3445, Perplexity: 209.4470, Accuracy: 14.14%\n",
            "Epoch 44/500, Loss: 5.3006, Perplexity: 200.4598, Accuracy: 14.14%\n",
            "Epoch 45/500, Loss: 5.3003, Perplexity: 200.4025, Accuracy: 13.13%\n",
            "Epoch 46/500, Loss: 5.2484, Perplexity: 190.2567, Accuracy: 13.13%\n",
            "Epoch 47/500, Loss: 5.2349, Perplexity: 187.7026, Accuracy: 14.14%\n",
            "Epoch 48/500, Loss: 5.2281, Perplexity: 186.4436, Accuracy: 14.14%\n",
            "Epoch 49/500, Loss: 5.1964, Perplexity: 180.6292, Accuracy: 14.14%\n",
            "Epoch 50/500, Loss: 5.1682, Perplexity: 175.5954, Accuracy: 15.15%\n",
            "Epoch 51/500, Loss: 5.1637, Perplexity: 174.8128, Accuracy: 14.14%\n",
            "Epoch 52/500, Loss: 5.1616, Perplexity: 174.4354, Accuracy: 13.13%\n",
            "Epoch 53/500, Loss: 5.1493, Perplexity: 172.3034, Accuracy: 14.14%\n",
            "Epoch 54/500, Loss: 5.1079, Perplexity: 165.3166, Accuracy: 14.14%\n",
            "Epoch 55/500, Loss: 5.0905, Perplexity: 162.4772, Accuracy: 16.16%\n",
            "Epoch 56/500, Loss: 5.0625, Perplexity: 157.9777, Accuracy: 14.14%\n",
            "Epoch 57/500, Loss: 5.0367, Perplexity: 153.9575, Accuracy: 16.16%\n",
            "Epoch 58/500, Loss: 5.0475, Perplexity: 155.6330, Accuracy: 15.15%\n",
            "Epoch 59/500, Loss: 5.0003, Perplexity: 148.4554, Accuracy: 16.16%\n",
            "Epoch 60/500, Loss: 5.0020, Perplexity: 148.7134, Accuracy: 16.16%\n",
            "Epoch 61/500, Loss: 4.9837, Perplexity: 146.0208, Accuracy: 16.16%\n",
            "Epoch 62/500, Loss: 4.9774, Perplexity: 145.1017, Accuracy: 17.17%\n",
            "Epoch 63/500, Loss: 4.9469, Perplexity: 140.7414, Accuracy: 15.15%\n",
            "Epoch 64/500, Loss: 4.9595, Perplexity: 142.5212, Accuracy: 16.16%\n",
            "Epoch 65/500, Loss: 4.9228, Perplexity: 137.3921, Accuracy: 17.17%\n",
            "Epoch 66/500, Loss: 4.9045, Perplexity: 134.8967, Accuracy: 18.18%\n",
            "Epoch 67/500, Loss: 4.8671, Perplexity: 129.9485, Accuracy: 17.17%\n",
            "Epoch 68/500, Loss: 4.8616, Perplexity: 129.2360, Accuracy: 17.17%\n",
            "Epoch 69/500, Loss: 4.8642, Perplexity: 129.5705, Accuracy: 19.19%\n",
            "Epoch 70/500, Loss: 4.8354, Perplexity: 125.8907, Accuracy: 18.18%\n",
            "Epoch 71/500, Loss: 4.8050, Perplexity: 122.1152, Accuracy: 22.22%\n",
            "Epoch 72/500, Loss: 4.7931, Perplexity: 120.6781, Accuracy: 19.19%\n",
            "Epoch 73/500, Loss: 4.7793, Perplexity: 119.0248, Accuracy: 18.18%\n",
            "Epoch 74/500, Loss: 4.7698, Perplexity: 117.8972, Accuracy: 23.23%\n",
            "Epoch 75/500, Loss: 4.7478, Perplexity: 115.3309, Accuracy: 21.21%\n",
            "Epoch 76/500, Loss: 4.7277, Perplexity: 113.0336, Accuracy: 24.24%\n",
            "Epoch 77/500, Loss: 4.6963, Perplexity: 109.5409, Accuracy: 24.24%\n",
            "Epoch 78/500, Loss: 4.7018, Perplexity: 110.1425, Accuracy: 26.26%\n",
            "Epoch 79/500, Loss: 4.7057, Perplexity: 110.5794, Accuracy: 24.24%\n",
            "Epoch 80/500, Loss: 4.6961, Perplexity: 109.5151, Accuracy: 24.24%\n",
            "Epoch 81/500, Loss: 4.6541, Perplexity: 105.0114, Accuracy: 25.25%\n",
            "Epoch 82/500, Loss: 4.6637, Perplexity: 106.0311, Accuracy: 24.24%\n",
            "Epoch 83/500, Loss: 4.6512, Perplexity: 104.7152, Accuracy: 25.25%\n",
            "Epoch 84/500, Loss: 4.6278, Perplexity: 102.2927, Accuracy: 26.26%\n",
            "Epoch 85/500, Loss: 4.5908, Perplexity: 98.5753, Accuracy: 26.26%\n",
            "Epoch 86/500, Loss: 4.5976, Perplexity: 99.2478, Accuracy: 25.25%\n",
            "Epoch 87/500, Loss: 4.5893, Perplexity: 98.4285, Accuracy: 28.28%\n",
            "Epoch 88/500, Loss: 4.5632, Perplexity: 95.8884, Accuracy: 26.26%\n",
            "Epoch 89/500, Loss: 4.5874, Perplexity: 98.2348, Accuracy: 27.27%\n",
            "Epoch 90/500, Loss: 4.5286, Perplexity: 92.6258, Accuracy: 28.28%\n",
            "Epoch 91/500, Loss: 4.5121, Perplexity: 91.1168, Accuracy: 27.27%\n",
            "Epoch 92/500, Loss: 4.5054, Perplexity: 90.5077, Accuracy: 29.29%\n",
            "Epoch 93/500, Loss: 4.5150, Perplexity: 91.3752, Accuracy: 29.29%\n",
            "Epoch 94/500, Loss: 4.4908, Perplexity: 89.1924, Accuracy: 29.29%\n",
            "Epoch 95/500, Loss: 4.4834, Perplexity: 88.5325, Accuracy: 30.30%\n",
            "Epoch 96/500, Loss: 4.4635, Perplexity: 86.7931, Accuracy: 29.29%\n",
            "Epoch 97/500, Loss: 4.4465, Perplexity: 85.3300, Accuracy: 30.30%\n",
            "Epoch 98/500, Loss: 4.4533, Perplexity: 85.9137, Accuracy: 29.29%\n",
            "Epoch 99/500, Loss: 4.4340, Perplexity: 84.2705, Accuracy: 29.29%\n",
            "Epoch 100/500, Loss: 4.4180, Perplexity: 82.9313, Accuracy: 29.29%\n",
            "Epoch 101/500, Loss: 4.4199, Perplexity: 83.0863, Accuracy: 31.31%\n",
            "Epoch 102/500, Loss: 4.3979, Perplexity: 81.2839, Accuracy: 32.32%\n",
            "Epoch 103/500, Loss: 4.3784, Perplexity: 79.7089, Accuracy: 32.32%\n",
            "Epoch 104/500, Loss: 4.3847, Perplexity: 80.2133, Accuracy: 32.32%\n",
            "Epoch 105/500, Loss: 4.3502, Perplexity: 77.4973, Accuracy: 29.29%\n",
            "Epoch 106/500, Loss: 4.3507, Perplexity: 77.5330, Accuracy: 33.33%\n",
            "Epoch 107/500, Loss: 4.3374, Perplexity: 76.5108, Accuracy: 33.33%\n",
            "Epoch 108/500, Loss: 4.3198, Perplexity: 75.1715, Accuracy: 33.33%\n",
            "Epoch 109/500, Loss: 4.3086, Perplexity: 74.3360, Accuracy: 34.34%\n",
            "Epoch 110/500, Loss: 4.3075, Perplexity: 74.2547, Accuracy: 36.36%\n",
            "Epoch 111/500, Loss: 4.2892, Perplexity: 72.9109, Accuracy: 33.33%\n",
            "Epoch 112/500, Loss: 4.2929, Perplexity: 73.1794, Accuracy: 33.33%\n",
            "Epoch 113/500, Loss: 4.2540, Perplexity: 70.3862, Accuracy: 36.36%\n",
            "Epoch 114/500, Loss: 4.2795, Perplexity: 72.2076, Accuracy: 35.35%\n",
            "Epoch 115/500, Loss: 4.2529, Perplexity: 70.3085, Accuracy: 34.34%\n",
            "Epoch 116/500, Loss: 4.2455, Perplexity: 69.7900, Accuracy: 35.35%\n",
            "Epoch 117/500, Loss: 4.2183, Perplexity: 67.9146, Accuracy: 39.39%\n",
            "Epoch 118/500, Loss: 4.2088, Perplexity: 67.2728, Accuracy: 38.38%\n",
            "Epoch 119/500, Loss: 4.2227, Perplexity: 68.2194, Accuracy: 38.38%\n",
            "Epoch 120/500, Loss: 4.1997, Perplexity: 66.6647, Accuracy: 36.36%\n",
            "Epoch 121/500, Loss: 4.1762, Perplexity: 65.1180, Accuracy: 40.40%\n",
            "Epoch 122/500, Loss: 4.1854, Perplexity: 65.7184, Accuracy: 41.41%\n",
            "Epoch 123/500, Loss: 4.1731, Perplexity: 64.9194, Accuracy: 37.37%\n",
            "Epoch 124/500, Loss: 4.1642, Perplexity: 64.3443, Accuracy: 42.42%\n",
            "Epoch 125/500, Loss: 4.1617, Perplexity: 64.1812, Accuracy: 40.40%\n",
            "Epoch 126/500, Loss: 4.1509, Perplexity: 63.4892, Accuracy: 43.43%\n",
            "Epoch 127/500, Loss: 4.1619, Perplexity: 64.1927, Accuracy: 44.44%\n",
            "Epoch 128/500, Loss: 4.1170, Perplexity: 61.3732, Accuracy: 44.44%\n",
            "Epoch 129/500, Loss: 4.1208, Perplexity: 61.6103, Accuracy: 40.40%\n",
            "Epoch 130/500, Loss: 4.1187, Perplexity: 61.4766, Accuracy: 40.40%\n",
            "Epoch 131/500, Loss: 4.0885, Perplexity: 59.6488, Accuracy: 43.43%\n",
            "Epoch 132/500, Loss: 4.0873, Perplexity: 59.5783, Accuracy: 41.41%\n",
            "Epoch 133/500, Loss: 4.0979, Perplexity: 60.2117, Accuracy: 39.39%\n",
            "Epoch 134/500, Loss: 4.0788, Perplexity: 59.0756, Accuracy: 44.44%\n",
            "Epoch 135/500, Loss: 4.0675, Perplexity: 58.4085, Accuracy: 43.43%\n",
            "Epoch 136/500, Loss: 4.0675, Perplexity: 58.4097, Accuracy: 49.49%\n",
            "Epoch 137/500, Loss: 4.0489, Perplexity: 57.3332, Accuracy: 42.42%\n",
            "Epoch 138/500, Loss: 4.0531, Perplexity: 57.5756, Accuracy: 45.45%\n",
            "Epoch 139/500, Loss: 4.0453, Perplexity: 57.1305, Accuracy: 44.44%\n",
            "Epoch 140/500, Loss: 4.0501, Perplexity: 57.4034, Accuracy: 45.45%\n",
            "Epoch 141/500, Loss: 4.0278, Perplexity: 56.1375, Accuracy: 41.41%\n",
            "Epoch 142/500, Loss: 4.0106, Perplexity: 55.1810, Accuracy: 48.48%\n",
            "Epoch 143/500, Loss: 4.0068, Perplexity: 54.9701, Accuracy: 50.51%\n",
            "Epoch 144/500, Loss: 4.0109, Perplexity: 55.1973, Accuracy: 47.47%\n",
            "Epoch 145/500, Loss: 3.9953, Perplexity: 54.3401, Accuracy: 46.46%\n",
            "Epoch 146/500, Loss: 3.9908, Perplexity: 54.0969, Accuracy: 45.45%\n",
            "Epoch 147/500, Loss: 3.9872, Perplexity: 53.9054, Accuracy: 45.45%\n",
            "Epoch 148/500, Loss: 3.9841, Perplexity: 53.7371, Accuracy: 53.54%\n",
            "Epoch 149/500, Loss: 3.9722, Perplexity: 53.1039, Accuracy: 55.56%\n",
            "Epoch 150/500, Loss: 3.9407, Perplexity: 51.4564, Accuracy: 54.55%\n",
            "Epoch 151/500, Loss: 3.9457, Perplexity: 51.7140, Accuracy: 52.53%\n",
            "Epoch 152/500, Loss: 3.9341, Perplexity: 51.1149, Accuracy: 52.53%\n",
            "Epoch 153/500, Loss: 3.9360, Perplexity: 51.2115, Accuracy: 58.59%\n",
            "Epoch 154/500, Loss: 3.9305, Perplexity: 50.9348, Accuracy: 53.54%\n",
            "Epoch 155/500, Loss: 3.9203, Perplexity: 50.4159, Accuracy: 53.54%\n",
            "Epoch 156/500, Loss: 3.9447, Perplexity: 51.6617, Accuracy: 52.53%\n",
            "Epoch 157/500, Loss: 3.9221, Perplexity: 50.5052, Accuracy: 54.55%\n",
            "Epoch 158/500, Loss: 3.9220, Perplexity: 50.5034, Accuracy: 51.52%\n",
            "Epoch 159/500, Loss: 3.8910, Perplexity: 48.9602, Accuracy: 55.56%\n",
            "Epoch 160/500, Loss: 3.8909, Perplexity: 48.9535, Accuracy: 55.56%\n",
            "Epoch 161/500, Loss: 3.8927, Perplexity: 49.0420, Accuracy: 54.55%\n",
            "Epoch 162/500, Loss: 3.9019, Perplexity: 49.4968, Accuracy: 53.54%\n",
            "Epoch 163/500, Loss: 3.8577, Perplexity: 47.3568, Accuracy: 58.59%\n",
            "Epoch 164/500, Loss: 3.8854, Perplexity: 48.6857, Accuracy: 56.57%\n",
            "Epoch 165/500, Loss: 3.8574, Perplexity: 47.3435, Accuracy: 56.57%\n",
            "Epoch 166/500, Loss: 3.8470, Perplexity: 46.8539, Accuracy: 57.58%\n",
            "Epoch 167/500, Loss: 3.8574, Perplexity: 47.3426, Accuracy: 59.60%\n",
            "Epoch 168/500, Loss: 3.8483, Perplexity: 46.9121, Accuracy: 56.57%\n",
            "Epoch 169/500, Loss: 3.8378, Perplexity: 46.4253, Accuracy: 56.57%\n",
            "Epoch 170/500, Loss: 3.8444, Perplexity: 46.7315, Accuracy: 55.56%\n",
            "Epoch 171/500, Loss: 3.8369, Perplexity: 46.3833, Accuracy: 56.57%\n",
            "Epoch 172/500, Loss: 3.8336, Perplexity: 46.2309, Accuracy: 57.58%\n",
            "Epoch 173/500, Loss: 3.8191, Perplexity: 45.5641, Accuracy: 56.57%\n",
            "Epoch 174/500, Loss: 3.7932, Perplexity: 44.4003, Accuracy: 56.57%\n",
            "Epoch 175/500, Loss: 3.8097, Perplexity: 45.1388, Accuracy: 57.58%\n",
            "Epoch 176/500, Loss: 3.8024, Perplexity: 44.8088, Accuracy: 60.61%\n",
            "Epoch 177/500, Loss: 3.8078, Perplexity: 45.0532, Accuracy: 62.63%\n",
            "Epoch 178/500, Loss: 3.7965, Perplexity: 44.5459, Accuracy: 57.58%\n",
            "Epoch 179/500, Loss: 3.8056, Perplexity: 44.9525, Accuracy: 62.63%\n",
            "Epoch 180/500, Loss: 3.7809, Perplexity: 43.8563, Accuracy: 58.59%\n",
            "Epoch 181/500, Loss: 3.7811, Perplexity: 43.8643, Accuracy: 59.60%\n",
            "Epoch 182/500, Loss: 3.7897, Perplexity: 44.2429, Accuracy: 58.59%\n",
            "Epoch 183/500, Loss: 3.7641, Perplexity: 43.1248, Accuracy: 58.59%\n",
            "Epoch 184/500, Loss: 3.7924, Perplexity: 44.3637, Accuracy: 58.59%\n",
            "Epoch 185/500, Loss: 3.7557, Perplexity: 42.7639, Accuracy: 60.61%\n",
            "Epoch 186/500, Loss: 3.7606, Perplexity: 42.9747, Accuracy: 57.58%\n",
            "Epoch 187/500, Loss: 3.7518, Perplexity: 42.5989, Accuracy: 60.61%\n",
            "Epoch 188/500, Loss: 3.7412, Perplexity: 42.1483, Accuracy: 61.62%\n",
            "Epoch 189/500, Loss: 3.7524, Perplexity: 42.6230, Accuracy: 63.64%\n",
            "Epoch 190/500, Loss: 3.7487, Perplexity: 42.4650, Accuracy: 62.63%\n",
            "Epoch 191/500, Loss: 3.7220, Perplexity: 41.3451, Accuracy: 61.62%\n",
            "Epoch 192/500, Loss: 3.7213, Perplexity: 41.3195, Accuracy: 65.66%\n",
            "Epoch 193/500, Loss: 3.7303, Perplexity: 41.6895, Accuracy: 64.65%\n",
            "Epoch 194/500, Loss: 3.7058, Perplexity: 40.6817, Accuracy: 62.63%\n",
            "Epoch 195/500, Loss: 3.7176, Perplexity: 41.1657, Accuracy: 65.66%\n",
            "Epoch 196/500, Loss: 3.7288, Perplexity: 41.6273, Accuracy: 58.59%\n",
            "Epoch 197/500, Loss: 3.6864, Perplexity: 39.9026, Accuracy: 64.65%\n",
            "Epoch 198/500, Loss: 3.7044, Perplexity: 40.6275, Accuracy: 64.65%\n",
            "Epoch 199/500, Loss: 3.7002, Perplexity: 40.4574, Accuracy: 63.64%\n",
            "Epoch 200/500, Loss: 3.6999, Perplexity: 40.4446, Accuracy: 68.69%\n",
            "Epoch 201/500, Loss: 3.6926, Perplexity: 40.1492, Accuracy: 65.66%\n",
            "Epoch 202/500, Loss: 3.6943, Perplexity: 40.2157, Accuracy: 63.64%\n",
            "Epoch 203/500, Loss: 3.6809, Perplexity: 39.6840, Accuracy: 62.63%\n",
            "Epoch 204/500, Loss: 3.6863, Perplexity: 39.8988, Accuracy: 64.65%\n",
            "Epoch 205/500, Loss: 3.6681, Perplexity: 39.1766, Accuracy: 65.66%\n",
            "Epoch 206/500, Loss: 3.6742, Perplexity: 39.4161, Accuracy: 63.64%\n",
            "Epoch 207/500, Loss: 3.6691, Perplexity: 39.2151, Accuracy: 64.65%\n",
            "Epoch 208/500, Loss: 3.6540, Perplexity: 38.6300, Accuracy: 60.61%\n",
            "Epoch 209/500, Loss: 3.6422, Perplexity: 38.1747, Accuracy: 66.67%\n",
            "Epoch 210/500, Loss: 3.6581, Perplexity: 38.7868, Accuracy: 63.64%\n",
            "Epoch 211/500, Loss: 3.6580, Perplexity: 38.7825, Accuracy: 68.69%\n",
            "Epoch 212/500, Loss: 3.6465, Perplexity: 38.3391, Accuracy: 65.66%\n",
            "Epoch 213/500, Loss: 3.6557, Perplexity: 38.6933, Accuracy: 66.67%\n",
            "Epoch 214/500, Loss: 3.6429, Perplexity: 38.2006, Accuracy: 67.68%\n",
            "Epoch 215/500, Loss: 3.6449, Perplexity: 38.2773, Accuracy: 67.68%\n",
            "Epoch 216/500, Loss: 3.6351, Perplexity: 37.9055, Accuracy: 66.67%\n",
            "Epoch 217/500, Loss: 3.6223, Perplexity: 37.4225, Accuracy: 65.66%\n",
            "Epoch 218/500, Loss: 3.6254, Perplexity: 37.5380, Accuracy: 68.69%\n",
            "Epoch 219/500, Loss: 3.6193, Perplexity: 37.3119, Accuracy: 65.66%\n",
            "Epoch 220/500, Loss: 3.6252, Perplexity: 37.5325, Accuracy: 65.66%\n",
            "Epoch 221/500, Loss: 3.6246, Perplexity: 37.5084, Accuracy: 63.64%\n",
            "Epoch 222/500, Loss: 3.6047, Perplexity: 36.7720, Accuracy: 67.68%\n",
            "Epoch 223/500, Loss: 3.6135, Perplexity: 37.0959, Accuracy: 66.67%\n",
            "Epoch 224/500, Loss: 3.6125, Perplexity: 37.0570, Accuracy: 67.68%\n",
            "Epoch 225/500, Loss: 3.5888, Perplexity: 36.1908, Accuracy: 62.63%\n",
            "Epoch 226/500, Loss: 3.6077, Perplexity: 36.8797, Accuracy: 67.68%\n",
            "Epoch 227/500, Loss: 3.6010, Perplexity: 36.6346, Accuracy: 66.67%\n",
            "Epoch 228/500, Loss: 3.6140, Perplexity: 37.1125, Accuracy: 66.67%\n",
            "Epoch 229/500, Loss: 3.5902, Perplexity: 36.2405, Accuracy: 68.69%\n",
            "Epoch 230/500, Loss: 3.5805, Perplexity: 35.8920, Accuracy: 68.69%\n",
            "Epoch 231/500, Loss: 3.5844, Perplexity: 36.0331, Accuracy: 67.68%\n",
            "Epoch 232/500, Loss: 3.5914, Perplexity: 36.2837, Accuracy: 65.66%\n",
            "Epoch 233/500, Loss: 3.5856, Perplexity: 36.0745, Accuracy: 65.66%\n",
            "Epoch 234/500, Loss: 3.5988, Perplexity: 36.5544, Accuracy: 68.69%\n",
            "Epoch 235/500, Loss: 3.5767, Perplexity: 35.7558, Accuracy: 67.68%\n",
            "Epoch 236/500, Loss: 3.5810, Perplexity: 35.9086, Accuracy: 69.70%\n",
            "Epoch 237/500, Loss: 3.5939, Perplexity: 36.3753, Accuracy: 65.66%\n",
            "Epoch 238/500, Loss: 3.5676, Perplexity: 35.4298, Accuracy: 70.71%\n",
            "Epoch 239/500, Loss: 3.5778, Perplexity: 35.7943, Accuracy: 68.69%\n",
            "Epoch 240/500, Loss: 3.5539, Perplexity: 34.9501, Accuracy: 69.70%\n",
            "Epoch 241/500, Loss: 3.5541, Perplexity: 34.9573, Accuracy: 69.70%\n",
            "Epoch 242/500, Loss: 3.5405, Perplexity: 34.4830, Accuracy: 66.67%\n",
            "Epoch 243/500, Loss: 3.5609, Perplexity: 35.1933, Accuracy: 70.71%\n",
            "Epoch 244/500, Loss: 3.5733, Perplexity: 35.6338, Accuracy: 68.69%\n",
            "Epoch 245/500, Loss: 3.5574, Perplexity: 35.0706, Accuracy: 66.67%\n",
            "Epoch 246/500, Loss: 3.5688, Perplexity: 35.4749, Accuracy: 65.66%\n",
            "Epoch 247/500, Loss: 3.5696, Perplexity: 35.5024, Accuracy: 68.69%\n",
            "Epoch 248/500, Loss: 3.5937, Perplexity: 36.3673, Accuracy: 66.67%\n",
            "Epoch 249/500, Loss: 3.5632, Perplexity: 35.2765, Accuracy: 71.72%\n",
            "Epoch 250/500, Loss: 3.5448, Perplexity: 34.6340, Accuracy: 67.68%\n",
            "Epoch 251/500, Loss: 3.5515, Perplexity: 34.8642, Accuracy: 67.68%\n",
            "Epoch 252/500, Loss: 3.5612, Perplexity: 35.2063, Accuracy: 66.67%\n",
            "Epoch 253/500, Loss: 3.5393, Perplexity: 34.4416, Accuracy: 66.67%\n",
            "Epoch 254/500, Loss: 3.5370, Perplexity: 34.3620, Accuracy: 66.67%\n",
            "Epoch 255/500, Loss: 3.5370, Perplexity: 34.3638, Accuracy: 68.69%\n",
            "Epoch 256/500, Loss: 3.5302, Perplexity: 34.1299, Accuracy: 65.66%\n",
            "Epoch 257/500, Loss: 3.5370, Perplexity: 34.3644, Accuracy: 68.69%\n",
            "Epoch 258/500, Loss: 3.5281, Perplexity: 34.0580, Accuracy: 67.68%\n",
            "Epoch 259/500, Loss: 3.5214, Perplexity: 33.8334, Accuracy: 66.67%\n",
            "Epoch 260/500, Loss: 3.5490, Perplexity: 34.7769, Accuracy: 67.68%\n",
            "Epoch 261/500, Loss: 3.5224, Perplexity: 33.8655, Accuracy: 67.68%\n",
            "Epoch 262/500, Loss: 3.5195, Perplexity: 33.7666, Accuracy: 68.69%\n",
            "Epoch 263/500, Loss: 3.5266, Perplexity: 34.0069, Accuracy: 69.70%\n",
            "Epoch 264/500, Loss: 3.5118, Perplexity: 33.5075, Accuracy: 71.72%\n",
            "Epoch 265/500, Loss: 3.5054, Perplexity: 33.2946, Accuracy: 68.69%\n",
            "Epoch 266/500, Loss: 3.5018, Perplexity: 33.1761, Accuracy: 70.71%\n",
            "Epoch 267/500, Loss: 3.5107, Perplexity: 33.4720, Accuracy: 69.70%\n",
            "Epoch 268/500, Loss: 3.4934, Perplexity: 32.8964, Accuracy: 69.70%\n",
            "Epoch 269/500, Loss: 3.5133, Perplexity: 33.5583, Accuracy: 71.72%\n",
            "Epoch 270/500, Loss: 3.5071, Perplexity: 33.3499, Accuracy: 68.69%\n",
            "Epoch 271/500, Loss: 3.5300, Perplexity: 34.1232, Accuracy: 69.70%\n",
            "Epoch 272/500, Loss: 3.5039, Perplexity: 33.2463, Accuracy: 69.70%\n",
            "Epoch 273/500, Loss: 3.4806, Perplexity: 32.4777, Accuracy: 69.70%\n",
            "Epoch 274/500, Loss: 3.4860, Perplexity: 32.6540, Accuracy: 68.69%\n",
            "Epoch 275/500, Loss: 3.5116, Perplexity: 33.5034, Accuracy: 70.71%\n",
            "Epoch 276/500, Loss: 3.5086, Perplexity: 33.4030, Accuracy: 67.68%\n",
            "Epoch 277/500, Loss: 3.4945, Perplexity: 32.9335, Accuracy: 69.70%\n",
            "Epoch 278/500, Loss: 3.4993, Perplexity: 33.0908, Accuracy: 67.68%\n",
            "Epoch 279/500, Loss: 3.4897, Perplexity: 32.7754, Accuracy: 69.70%\n",
            "Epoch 280/500, Loss: 3.4793, Perplexity: 32.4367, Accuracy: 68.69%\n",
            "Epoch 281/500, Loss: 3.4928, Perplexity: 32.8772, Accuracy: 69.70%\n",
            "Epoch 282/500, Loss: 3.4919, Perplexity: 32.8492, Accuracy: 69.70%\n",
            "Epoch 283/500, Loss: 3.4782, Perplexity: 32.4007, Accuracy: 70.71%\n",
            "Epoch 284/500, Loss: 3.4729, Perplexity: 32.2300, Accuracy: 70.71%\n",
            "Epoch 285/500, Loss: 3.4887, Perplexity: 32.7434, Accuracy: 72.73%\n",
            "Epoch 286/500, Loss: 3.4698, Perplexity: 32.1303, Accuracy: 69.70%\n",
            "Epoch 287/500, Loss: 3.4562, Perplexity: 31.6979, Accuracy: 71.72%\n",
            "Epoch 288/500, Loss: 3.4920, Perplexity: 32.8517, Accuracy: 68.69%\n",
            "Epoch 289/500, Loss: 3.4613, Perplexity: 31.8571, Accuracy: 73.74%\n",
            "Epoch 290/500, Loss: 3.4810, Perplexity: 32.4926, Accuracy: 69.70%\n",
            "Epoch 291/500, Loss: 3.4842, Perplexity: 32.5965, Accuracy: 69.70%\n",
            "Epoch 292/500, Loss: 3.4735, Perplexity: 32.2494, Accuracy: 72.73%\n",
            "Epoch 293/500, Loss: 3.4438, Perplexity: 31.3056, Accuracy: 69.70%\n",
            "Epoch 294/500, Loss: 3.4746, Perplexity: 32.2861, Accuracy: 70.71%\n",
            "Epoch 295/500, Loss: 3.4697, Perplexity: 32.1284, Accuracy: 68.69%\n",
            "Epoch 296/500, Loss: 3.4839, Perplexity: 32.5880, Accuracy: 69.70%\n",
            "Epoch 297/500, Loss: 3.4677, Perplexity: 32.0621, Accuracy: 70.71%\n",
            "Epoch 298/500, Loss: 3.4841, Perplexity: 32.5938, Accuracy: 69.70%\n",
            "Epoch 299/500, Loss: 3.4651, Perplexity: 31.9784, Accuracy: 67.68%\n",
            "Epoch 300/500, Loss: 3.4780, Perplexity: 32.3951, Accuracy: 69.70%\n",
            "Epoch 301/500, Loss: 3.4544, Perplexity: 31.6390, Accuracy: 70.71%\n",
            "Epoch 302/500, Loss: 3.4583, Perplexity: 31.7619, Accuracy: 67.68%\n",
            "Epoch 303/500, Loss: 3.4502, Perplexity: 31.5066, Accuracy: 70.71%\n",
            "Epoch 304/500, Loss: 3.4503, Perplexity: 31.5110, Accuracy: 72.73%\n",
            "Epoch 305/500, Loss: 3.4511, Perplexity: 31.5339, Accuracy: 70.71%\n",
            "Epoch 306/500, Loss: 3.4621, Perplexity: 31.8839, Accuracy: 69.70%\n",
            "Epoch 307/500, Loss: 3.4561, Perplexity: 31.6924, Accuracy: 68.69%\n",
            "Epoch 308/500, Loss: 3.4322, Perplexity: 30.9442, Accuracy: 70.71%\n",
            "Epoch 309/500, Loss: 3.4630, Perplexity: 31.9141, Accuracy: 71.72%\n",
            "Epoch 310/500, Loss: 3.4651, Perplexity: 31.9795, Accuracy: 68.69%\n",
            "Epoch 311/500, Loss: 3.4416, Perplexity: 31.2370, Accuracy: 70.71%\n",
            "Epoch 312/500, Loss: 3.4471, Perplexity: 31.4083, Accuracy: 70.71%\n",
            "Epoch 313/500, Loss: 3.4465, Perplexity: 31.3909, Accuracy: 70.71%\n",
            "Epoch 314/500, Loss: 3.4402, Perplexity: 31.1936, Accuracy: 69.70%\n",
            "Epoch 315/500, Loss: 3.4343, Perplexity: 31.0083, Accuracy: 72.73%\n",
            "Epoch 316/500, Loss: 3.4396, Perplexity: 31.1752, Accuracy: 68.69%\n",
            "Epoch 317/500, Loss: 3.4402, Perplexity: 31.1939, Accuracy: 70.71%\n",
            "Epoch 318/500, Loss: 3.4324, Perplexity: 30.9519, Accuracy: 71.72%\n",
            "Epoch 319/500, Loss: 3.4221, Perplexity: 30.6332, Accuracy: 69.70%\n",
            "Epoch 320/500, Loss: 3.4253, Perplexity: 30.7327, Accuracy: 73.74%\n",
            "Epoch 321/500, Loss: 3.4359, Perplexity: 31.0605, Accuracy: 71.72%\n",
            "Epoch 322/500, Loss: 3.4260, Perplexity: 30.7543, Accuracy: 71.72%\n",
            "Epoch 323/500, Loss: 3.4351, Perplexity: 31.0350, Accuracy: 73.74%\n",
            "Epoch 324/500, Loss: 3.4360, Perplexity: 31.0625, Accuracy: 70.71%\n",
            "Epoch 325/500, Loss: 3.4409, Perplexity: 31.2146, Accuracy: 68.69%\n",
            "Epoch 326/500, Loss: 3.4410, Perplexity: 31.2178, Accuracy: 68.69%\n",
            "Epoch 327/500, Loss: 3.4164, Perplexity: 30.4583, Accuracy: 70.71%\n",
            "Epoch 328/500, Loss: 3.4351, Perplexity: 31.0351, Accuracy: 70.71%\n",
            "Epoch 329/500, Loss: 3.4476, Perplexity: 31.4250, Accuracy: 70.71%\n",
            "Epoch 330/500, Loss: 3.4114, Perplexity: 30.3069, Accuracy: 74.75%\n",
            "Epoch 331/500, Loss: 3.4077, Perplexity: 30.1957, Accuracy: 70.71%\n",
            "Epoch 332/500, Loss: 3.4280, Perplexity: 30.8161, Accuracy: 72.73%\n",
            "Epoch 333/500, Loss: 3.4214, Perplexity: 30.6124, Accuracy: 71.72%\n",
            "Epoch 334/500, Loss: 3.4135, Perplexity: 30.3716, Accuracy: 72.73%\n",
            "Epoch 335/500, Loss: 3.4171, Perplexity: 30.4798, Accuracy: 71.72%\n",
            "Epoch 336/500, Loss: 3.4189, Perplexity: 30.5371, Accuracy: 69.70%\n",
            "Epoch 337/500, Loss: 3.4353, Perplexity: 31.0402, Accuracy: 72.73%\n",
            "Epoch 338/500, Loss: 3.4184, Perplexity: 30.5193, Accuracy: 71.72%\n",
            "Epoch 339/500, Loss: 3.4110, Perplexity: 30.2945, Accuracy: 71.72%\n",
            "Epoch 340/500, Loss: 3.4133, Perplexity: 30.3646, Accuracy: 72.73%\n",
            "Epoch 341/500, Loss: 3.4270, Perplexity: 30.7841, Accuracy: 69.70%\n",
            "Epoch 342/500, Loss: 3.3964, Perplexity: 29.8557, Accuracy: 72.73%\n",
            "Epoch 343/500, Loss: 3.4075, Perplexity: 30.1890, Accuracy: 70.71%\n",
            "Epoch 344/500, Loss: 3.4158, Perplexity: 30.4407, Accuracy: 69.70%\n",
            "Epoch 345/500, Loss: 3.4055, Perplexity: 30.1292, Accuracy: 72.73%\n",
            "Epoch 346/500, Loss: 3.4283, Perplexity: 30.8234, Accuracy: 70.71%\n",
            "Epoch 347/500, Loss: 3.4366, Perplexity: 31.0805, Accuracy: 69.70%\n",
            "Epoch 348/500, Loss: 3.4043, Perplexity: 30.0926, Accuracy: 73.74%\n",
            "Epoch 349/500, Loss: 3.4017, Perplexity: 30.0151, Accuracy: 72.73%\n",
            "Epoch 350/500, Loss: 3.3869, Perplexity: 29.5732, Accuracy: 72.73%\n",
            "Epoch 351/500, Loss: 3.4068, Perplexity: 30.1676, Accuracy: 70.71%\n",
            "Epoch 352/500, Loss: 3.4344, Perplexity: 31.0134, Accuracy: 73.74%\n",
            "Epoch 353/500, Loss: 3.4155, Perplexity: 30.4330, Accuracy: 70.71%\n",
            "Epoch 354/500, Loss: 3.4115, Perplexity: 30.3102, Accuracy: 71.72%\n",
            "Epoch 355/500, Loss: 3.4112, Perplexity: 30.3012, Accuracy: 69.70%\n",
            "Epoch 356/500, Loss: 3.3838, Perplexity: 29.4822, Accuracy: 69.70%\n",
            "Epoch 357/500, Loss: 3.4214, Perplexity: 30.6125, Accuracy: 74.75%\n",
            "Epoch 358/500, Loss: 3.4132, Perplexity: 30.3628, Accuracy: 69.70%\n",
            "Epoch 359/500, Loss: 3.3926, Perplexity: 29.7436, Accuracy: 71.72%\n",
            "Epoch 360/500, Loss: 3.4188, Perplexity: 30.5320, Accuracy: 71.72%\n",
            "Epoch 361/500, Loss: 3.3980, Perplexity: 29.9034, Accuracy: 72.73%\n",
            "Epoch 362/500, Loss: 3.3974, Perplexity: 29.8862, Accuracy: 71.72%\n",
            "Epoch 363/500, Loss: 3.4128, Perplexity: 30.3490, Accuracy: 70.71%\n",
            "Epoch 364/500, Loss: 3.3824, Perplexity: 29.4404, Accuracy: 71.72%\n",
            "Epoch 365/500, Loss: 3.4021, Perplexity: 30.0269, Accuracy: 71.72%\n",
            "Epoch 366/500, Loss: 3.3992, Perplexity: 29.9393, Accuracy: 71.72%\n",
            "Epoch 367/500, Loss: 3.4070, Perplexity: 30.1752, Accuracy: 70.71%\n",
            "Epoch 368/500, Loss: 3.3977, Perplexity: 29.8954, Accuracy: 73.74%\n",
            "Epoch 369/500, Loss: 3.4095, Perplexity: 30.2509, Accuracy: 72.73%\n",
            "Epoch 370/500, Loss: 3.3816, Perplexity: 29.4183, Accuracy: 74.75%\n",
            "Epoch 371/500, Loss: 3.3881, Perplexity: 29.6108, Accuracy: 70.71%\n",
            "Epoch 372/500, Loss: 3.3719, Perplexity: 29.1330, Accuracy: 73.74%\n",
            "Epoch 373/500, Loss: 3.3789, Perplexity: 29.3397, Accuracy: 73.74%\n",
            "Epoch 374/500, Loss: 3.3973, Perplexity: 29.8832, Accuracy: 72.73%\n",
            "Epoch 375/500, Loss: 3.3811, Perplexity: 29.4026, Accuracy: 72.73%\n",
            "Epoch 376/500, Loss: 3.4144, Perplexity: 30.3998, Accuracy: 72.73%\n",
            "Epoch 377/500, Loss: 3.3777, Perplexity: 29.3045, Accuracy: 67.68%\n",
            "Epoch 378/500, Loss: 3.3909, Perplexity: 29.6930, Accuracy: 72.73%\n",
            "Epoch 379/500, Loss: 3.3884, Perplexity: 29.6180, Accuracy: 69.70%\n",
            "Epoch 380/500, Loss: 3.3776, Perplexity: 29.3002, Accuracy: 72.73%\n",
            "Epoch 381/500, Loss: 3.3931, Perplexity: 29.7573, Accuracy: 71.72%\n",
            "Epoch 382/500, Loss: 3.3916, Perplexity: 29.7122, Accuracy: 72.73%\n",
            "Epoch 383/500, Loss: 3.4011, Perplexity: 29.9971, Accuracy: 74.75%\n",
            "Epoch 384/500, Loss: 3.4009, Perplexity: 29.9915, Accuracy: 72.73%\n",
            "Epoch 385/500, Loss: 3.3966, Perplexity: 29.8627, Accuracy: 72.73%\n",
            "Epoch 386/500, Loss: 3.3996, Perplexity: 29.9518, Accuracy: 72.73%\n",
            "Epoch 387/500, Loss: 3.3721, Perplexity: 29.1392, Accuracy: 73.74%\n",
            "Epoch 388/500, Loss: 3.3951, Perplexity: 29.8165, Accuracy: 70.71%\n",
            "Epoch 389/500, Loss: 3.3794, Perplexity: 29.3541, Accuracy: 71.72%\n",
            "Epoch 390/500, Loss: 3.3618, Perplexity: 28.8410, Accuracy: 71.72%\n",
            "Epoch 391/500, Loss: 3.3845, Perplexity: 29.5036, Accuracy: 76.77%\n",
            "Epoch 392/500, Loss: 3.3786, Perplexity: 29.3286, Accuracy: 72.73%\n",
            "Epoch 393/500, Loss: 3.3855, Perplexity: 29.5337, Accuracy: 68.69%\n",
            "Epoch 394/500, Loss: 3.4021, Perplexity: 30.0274, Accuracy: 71.72%\n",
            "Epoch 395/500, Loss: 3.3831, Perplexity: 29.4632, Accuracy: 73.74%\n",
            "Epoch 396/500, Loss: 3.3902, Perplexity: 29.6717, Accuracy: 73.74%\n",
            "Epoch 397/500, Loss: 3.3973, Perplexity: 29.8847, Accuracy: 71.72%\n",
            "Epoch 398/500, Loss: 3.3847, Perplexity: 29.5103, Accuracy: 69.70%\n",
            "Epoch 399/500, Loss: 3.3748, Perplexity: 29.2183, Accuracy: 72.73%\n",
            "Epoch 400/500, Loss: 3.3885, Perplexity: 29.6217, Accuracy: 72.73%\n",
            "Epoch 401/500, Loss: 3.3690, Perplexity: 29.0503, Accuracy: 73.74%\n",
            "Epoch 402/500, Loss: 3.3864, Perplexity: 29.5590, Accuracy: 72.73%\n",
            "Epoch 403/500, Loss: 3.3794, Perplexity: 29.3542, Accuracy: 70.71%\n",
            "Epoch 404/500, Loss: 3.3642, Perplexity: 28.9095, Accuracy: 73.74%\n",
            "Epoch 405/500, Loss: 3.3768, Perplexity: 29.2761, Accuracy: 71.72%\n",
            "Epoch 406/500, Loss: 3.3889, Perplexity: 29.6322, Accuracy: 69.70%\n",
            "Epoch 407/500, Loss: 3.3887, Perplexity: 29.6278, Accuracy: 70.71%\n",
            "Epoch 408/500, Loss: 3.3892, Perplexity: 29.6425, Accuracy: 71.72%\n",
            "Epoch 409/500, Loss: 3.3711, Perplexity: 29.1093, Accuracy: 69.70%\n",
            "Epoch 410/500, Loss: 3.3912, Perplexity: 29.7014, Accuracy: 73.74%\n",
            "Epoch 411/500, Loss: 3.3550, Perplexity: 28.6446, Accuracy: 73.74%\n",
            "Epoch 412/500, Loss: 3.3872, Perplexity: 29.5843, Accuracy: 74.75%\n",
            "Epoch 413/500, Loss: 3.3822, Perplexity: 29.4344, Accuracy: 70.71%\n",
            "Epoch 414/500, Loss: 3.3892, Perplexity: 29.6418, Accuracy: 73.74%\n",
            "Epoch 415/500, Loss: 3.3723, Perplexity: 29.1451, Accuracy: 70.71%\n",
            "Epoch 416/500, Loss: 3.3680, Perplexity: 29.0212, Accuracy: 72.73%\n",
            "Epoch 417/500, Loss: 3.3808, Perplexity: 29.3941, Accuracy: 69.70%\n",
            "Epoch 418/500, Loss: 3.3716, Perplexity: 29.1239, Accuracy: 72.73%\n",
            "Epoch 419/500, Loss: 3.3649, Perplexity: 28.9299, Accuracy: 72.73%\n",
            "Epoch 420/500, Loss: 3.3630, Perplexity: 28.8749, Accuracy: 70.71%\n",
            "Epoch 421/500, Loss: 3.3591, Perplexity: 28.7619, Accuracy: 73.74%\n",
            "Epoch 422/500, Loss: 3.3654, Perplexity: 28.9442, Accuracy: 73.74%\n",
            "Epoch 423/500, Loss: 3.3666, Perplexity: 28.9786, Accuracy: 70.71%\n",
            "Epoch 424/500, Loss: 3.3760, Perplexity: 29.2537, Accuracy: 71.72%\n",
            "Epoch 425/500, Loss: 3.3718, Perplexity: 29.1297, Accuracy: 71.72%\n",
            "Epoch 426/500, Loss: 3.3861, Perplexity: 29.5497, Accuracy: 70.71%\n",
            "Epoch 427/500, Loss: 3.3561, Perplexity: 28.6759, Accuracy: 70.71%\n",
            "Epoch 428/500, Loss: 3.3643, Perplexity: 28.9145, Accuracy: 71.72%\n",
            "Epoch 429/500, Loss: 3.3681, Perplexity: 29.0219, Accuracy: 73.74%\n",
            "Epoch 430/500, Loss: 3.3709, Perplexity: 29.1041, Accuracy: 71.72%\n",
            "Epoch 431/500, Loss: 3.3602, Perplexity: 28.7949, Accuracy: 75.76%\n",
            "Epoch 432/500, Loss: 3.3917, Perplexity: 29.7165, Accuracy: 71.72%\n",
            "Epoch 433/500, Loss: 3.3596, Perplexity: 28.7768, Accuracy: 71.72%\n",
            "Epoch 434/500, Loss: 3.3518, Perplexity: 28.5546, Accuracy: 70.71%\n",
            "Epoch 435/500, Loss: 3.3523, Perplexity: 28.5678, Accuracy: 71.72%\n",
            "Epoch 436/500, Loss: 3.3574, Perplexity: 28.7140, Accuracy: 72.73%\n",
            "Epoch 437/500, Loss: 3.3789, Perplexity: 29.3394, Accuracy: 75.76%\n",
            "Epoch 438/500, Loss: 3.3798, Perplexity: 29.3647, Accuracy: 72.73%\n",
            "Epoch 439/500, Loss: 3.3679, Perplexity: 29.0168, Accuracy: 75.76%\n",
            "Epoch 440/500, Loss: 3.3469, Perplexity: 28.4158, Accuracy: 69.70%\n",
            "Epoch 441/500, Loss: 3.3763, Perplexity: 29.2637, Accuracy: 71.72%\n",
            "Epoch 442/500, Loss: 3.3800, Perplexity: 29.3711, Accuracy: 70.71%\n",
            "Epoch 443/500, Loss: 3.3702, Perplexity: 29.0844, Accuracy: 74.75%\n",
            "Epoch 444/500, Loss: 3.3601, Perplexity: 28.7911, Accuracy: 69.70%\n",
            "Epoch 445/500, Loss: 3.3470, Perplexity: 28.4174, Accuracy: 71.72%\n",
            "Epoch 446/500, Loss: 3.3556, Perplexity: 28.6633, Accuracy: 72.73%\n",
            "Epoch 447/500, Loss: 3.3812, Perplexity: 29.4056, Accuracy: 70.71%\n",
            "Epoch 448/500, Loss: 3.3509, Perplexity: 28.5277, Accuracy: 74.75%\n",
            "Epoch 449/500, Loss: 3.3669, Perplexity: 28.9897, Accuracy: 73.74%\n",
            "Epoch 450/500, Loss: 3.3605, Perplexity: 28.8024, Accuracy: 72.73%\n",
            "Epoch 451/500, Loss: 3.3613, Perplexity: 28.8274, Accuracy: 73.74%\n",
            "Epoch 452/500, Loss: 3.3898, Perplexity: 29.6614, Accuracy: 71.72%\n",
            "Epoch 453/500, Loss: 3.3813, Perplexity: 29.4094, Accuracy: 71.72%\n",
            "Epoch 454/500, Loss: 3.3839, Perplexity: 29.4858, Accuracy: 69.70%\n",
            "Epoch 455/500, Loss: 3.3685, Perplexity: 29.0350, Accuracy: 72.73%\n",
            "Epoch 456/500, Loss: 3.3479, Perplexity: 28.4420, Accuracy: 75.76%\n",
            "Epoch 457/500, Loss: 3.3670, Perplexity: 28.9926, Accuracy: 74.75%\n",
            "Epoch 458/500, Loss: 3.3543, Perplexity: 28.6258, Accuracy: 75.76%\n",
            "Epoch 459/500, Loss: 3.3632, Perplexity: 28.8829, Accuracy: 69.70%\n",
            "Epoch 460/500, Loss: 3.3634, Perplexity: 28.8878, Accuracy: 70.71%\n",
            "Epoch 461/500, Loss: 3.3464, Perplexity: 28.3993, Accuracy: 71.72%\n",
            "Epoch 462/500, Loss: 3.3586, Perplexity: 28.7488, Accuracy: 74.75%\n",
            "Epoch 463/500, Loss: 3.3641, Perplexity: 28.9073, Accuracy: 74.75%\n",
            "Epoch 464/500, Loss: 3.3657, Perplexity: 28.9547, Accuracy: 72.73%\n",
            "Epoch 465/500, Loss: 3.3587, Perplexity: 28.7530, Accuracy: 75.76%\n",
            "Epoch 466/500, Loss: 3.3511, Perplexity: 28.5353, Accuracy: 74.75%\n",
            "Epoch 467/500, Loss: 3.3538, Perplexity: 28.6103, Accuracy: 71.72%\n",
            "Epoch 468/500, Loss: 3.3580, Perplexity: 28.7303, Accuracy: 71.72%\n",
            "Epoch 469/500, Loss: 3.3590, Perplexity: 28.7611, Accuracy: 74.75%\n",
            "Epoch 470/500, Loss: 3.3754, Perplexity: 29.2345, Accuracy: 71.72%\n",
            "Epoch 471/500, Loss: 3.3521, Perplexity: 28.5618, Accuracy: 73.74%\n",
            "Epoch 472/500, Loss: 3.3494, Perplexity: 28.4847, Accuracy: 73.74%\n",
            "Epoch 473/500, Loss: 3.3446, Perplexity: 28.3492, Accuracy: 71.72%\n",
            "Epoch 474/500, Loss: 3.3380, Perplexity: 28.1627, Accuracy: 72.73%\n",
            "Epoch 475/500, Loss: 3.3507, Perplexity: 28.5230, Accuracy: 73.74%\n",
            "Epoch 476/500, Loss: 3.3519, Perplexity: 28.5581, Accuracy: 72.73%\n",
            "Epoch 477/500, Loss: 3.3440, Perplexity: 28.3325, Accuracy: 73.74%\n",
            "Epoch 478/500, Loss: 3.3398, Perplexity: 28.2148, Accuracy: 77.78%\n",
            "Epoch 479/500, Loss: 3.3771, Perplexity: 29.2863, Accuracy: 72.73%\n",
            "Epoch 480/500, Loss: 3.3611, Perplexity: 28.8216, Accuracy: 70.71%\n",
            "Epoch 481/500, Loss: 3.3264, Perplexity: 27.8371, Accuracy: 73.74%\n",
            "Epoch 482/500, Loss: 3.3769, Perplexity: 29.2794, Accuracy: 74.75%\n",
            "Epoch 483/500, Loss: 3.3543, Perplexity: 28.6258, Accuracy: 72.73%\n",
            "Epoch 484/500, Loss: 3.3394, Perplexity: 28.2034, Accuracy: 74.75%\n",
            "Epoch 485/500, Loss: 3.3833, Perplexity: 29.4666, Accuracy: 70.71%\n",
            "Epoch 486/500, Loss: 3.3597, Perplexity: 28.7795, Accuracy: 72.73%\n",
            "Epoch 487/500, Loss: 3.3304, Perplexity: 27.9487, Accuracy: 72.73%\n",
            "Epoch 488/500, Loss: 3.3545, Perplexity: 28.6313, Accuracy: 75.76%\n",
            "Epoch 489/500, Loss: 3.3740, Perplexity: 29.1952, Accuracy: 73.74%\n",
            "Epoch 490/500, Loss: 3.3788, Perplexity: 29.3349, Accuracy: 70.71%\n",
            "Epoch 491/500, Loss: 3.3643, Perplexity: 28.9119, Accuracy: 73.74%\n",
            "Epoch 492/500, Loss: 3.3520, Perplexity: 28.5606, Accuracy: 70.71%\n",
            "Epoch 493/500, Loss: 3.3540, Perplexity: 28.6181, Accuracy: 73.74%\n",
            "Epoch 494/500, Loss: 3.3580, Perplexity: 28.7318, Accuracy: 74.75%\n",
            "Epoch 495/500, Loss: 3.3349, Perplexity: 28.0749, Accuracy: 75.76%\n",
            "Epoch 496/500, Loss: 3.3647, Perplexity: 28.9241, Accuracy: 73.74%\n",
            "Epoch 497/500, Loss: 3.3707, Perplexity: 29.0975, Accuracy: 72.73%\n",
            "Epoch 498/500, Loss: 3.3562, Perplexity: 28.6803, Accuracy: 71.72%\n",
            "Epoch 499/500, Loss: 3.3656, Perplexity: 28.9515, Accuracy: 69.70%\n",
            "Epoch 500/500, Loss: 3.3611, Perplexity: 28.8214, Accuracy: 73.74%\n",
            "You: Bangladesh, located in\n",
            "Bot: , a country , and . , is , a dynamic burma , burma , a country asia , under political , and the , and the . is the on , the , and the global it the , and the south the , heritage , historical , and\n",
            "You: bangladesh\n",
            "Bot: , is , a struggles , is and a , is , is a dynamic vibrant , is the is with a country novels the , is cultural heritage , and the cost . of the , and a . it it the , and its , . with ,\n"
          ]
        }
      ]
    }
  ]
}