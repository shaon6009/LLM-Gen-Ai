{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10964883,"sourceType":"datasetVersion","datasetId":6822003},{"sourceId":55044584,"sourceType":"kernelVersion"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# the nlp_utils.py Part ","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nimport numpy as np\nfrom nltk.stem.porter import PorterStemmer\nstemmer= PorterStemmer()\n\ndef tokenize(sentence):\n    return nltk.word_tokenize(sentence)\n\ndef stem(word):\n    return stemmer.stem(word.lower())\n\ndef bag_of_words(tokenized_sentence, words):\n    sentence_words = [stem(word) for word in tokenized_sentence]\n    bag = np.zeros(len(words), dtype=np.float32)\n    for idx, w in enumerate(words):\n        if w in sentence_words: \n            bag[idx] = 1\n\n    return bag","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T21:32:42.430802Z","iopub.execute_input":"2025-03-08T21:32:42.431308Z","iopub.status.idle":"2025-03-08T21:32:42.444161Z","shell.execute_reply.started":"2025-03-08T21:32:42.431228Z","shell.execute_reply":"2025-03-08T21:32:42.442753Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\nclass NeuralNet(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(NeuralNet, self).__init__()\n        self.l1 = nn.Linear(input_size, hidden_size) \n        self.l2 = nn.Linear(hidden_size, hidden_size) \n        self.l3 = nn.Linear(hidden_size, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        out = self.l1(x)\n        out = self.relu(out)\n        out = self.l2(out)\n        out = self.relu(out)\n        out = self.l3(out)\n        # no activation and no softmax at the end\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T21:32:42.446169Z","iopub.execute_input":"2025-03-08T21:32:42.446720Z","iopub.status.idle":"2025-03-08T21:32:42.468897Z","shell.execute_reply.started":"2025-03-08T21:32:42.446673Z","shell.execute_reply":"2025-03-08T21:32:42.467635Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import numpy as np\nimport random\nimport json\n\nfrom nltk.stem.porter import PorterStemmer\n\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-08T21:32:42.472147Z","iopub.execute_input":"2025-03-08T21:32:42.472599Z","iopub.status.idle":"2025-03-08T21:32:42.498308Z","shell.execute_reply.started":"2025-03-08T21:32:42.472566Z","shell.execute_reply":"2025-03-08T21:32:42.496902Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"with open('/kaggle/input/chatbot1/intents.json', 'r') as f:\n    intents = json.load(f)\n    \nprint(intents)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T21:32:42.500212Z","iopub.execute_input":"2025-03-08T21:32:42.500695Z","iopub.status.idle":"2025-03-08T21:32:42.535046Z","shell.execute_reply.started":"2025-03-08T21:32:42.500656Z","shell.execute_reply":"2025-03-08T21:32:42.533773Z"}},"outputs":[{"name":"stdout","text":"{'intents': [{'tag': 'greeting', 'patterns': ['Hi', 'Hey', 'How are you', 'Is anyone there?', 'Hello', 'Good day'], 'responses': ['Hey :-)', 'Hello, thanks for visiting', 'Hi there, what can I do for you?', 'Hi there, how can I help?']}, {'tag': 'goodbye', 'patterns': ['Bye', 'See you later', 'Goodbye'], 'responses': ['See you later, thanks for visiting', 'Have a nice day', 'Bye! Come back again soon.']}, {'tag': 'thanks', 'patterns': ['Thanks', 'Thank you', \"That's helpful\", \"Thank's a lot!\"], 'responses': ['Happy to help!', 'Any time!', 'My pleasure']}, {'tag': 'items', 'patterns': ['Which items do you have?', 'What kinds of items are there?', 'What do you sell?'], 'responses': ['We sell coffee and tea', 'We have coffee and tea']}, {'tag': 'payments', 'patterns': ['Do you take credit cards?', 'Do you accept Mastercard?', 'Can I pay with Paypal?', 'Are you cash only?'], 'responses': ['We accept VISA, Mastercard and Paypal', 'We accept most major credit cards, and Paypal']}, {'tag': 'delivery', 'patterns': ['How long does delivery take?', 'How long does shipping take?', 'When do I get my delivery?'], 'responses': ['Delivery takes 2-4 days', 'Shipping takes 2-4 days']}, {'tag': 'funny', 'patterns': ['Tell me a joke!', 'Tell me something funny!', 'Do you know a joke?'], 'responses': ['Why did the hipster burn his mouth? He drank the coffee before it was cool.', 'What did the buffalo say when his son left for college? Bison.']}]}\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"all_words = []\ntags = []\nxy = []\n\nfor intent in intents['intents']:\n    tag = intent['tag']\n    tags.append(tag)\n    for pattern in intent['patterns']:\n        w = tokenize(pattern)\n        all_words.extend(w)\n        xy.append((w, tag))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T21:32:42.536157Z","iopub.execute_input":"2025-03-08T21:32:42.536572Z","iopub.status.idle":"2025-03-08T21:32:42.545479Z","shell.execute_reply.started":"2025-03-08T21:32:42.536536Z","shell.execute_reply":"2025-03-08T21:32:42.544024Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"ignore_words = ['?', '.', '!']\nall_words = [stem(w) for w in all_words if w not in ignore_words]\nall_words = sorted(set(all_words))\ntags = sorted(set(tags))\n\nprint(len(xy), \"patterns\")\nprint(len(tags), \"tags:\", tags)\nprint(len(all_words), \"unique stemmed words:\", all_words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T21:32:42.546664Z","iopub.execute_input":"2025-03-08T21:32:42.546990Z","iopub.status.idle":"2025-03-08T21:32:42.571487Z","shell.execute_reply.started":"2025-03-08T21:32:42.546965Z","shell.execute_reply":"2025-03-08T21:32:42.570020Z"}},"outputs":[{"name":"stdout","text":"26 patterns\n7 tags: ['delivery', 'funny', 'goodbye', 'greeting', 'items', 'payments', 'thanks']\n54 unique stemmed words: [\"'s\", 'a', 'accept', 'anyon', 'are', 'bye', 'can', 'card', 'cash', 'credit', 'day', 'deliveri', 'do', 'doe', 'funni', 'get', 'good', 'goodby', 'have', 'hello', 'help', 'hey', 'hi', 'how', 'i', 'is', 'item', 'joke', 'kind', 'know', 'later', 'long', 'lot', 'mastercard', 'me', 'my', 'of', 'onli', 'pay', 'paypal', 'see', 'sell', 'ship', 'someth', 'take', 'tell', 'thank', 'that', 'there', 'what', 'when', 'which', 'with', 'you']\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"X_train = []\ny_train = []\nfor (pattern_sentence, tag) in xy:\n    bag = bag_of_words(pattern_sentence, all_words)\n    X_train.append(bag)\n    label = tags.index(tag)\n    y_train.append(label)\n\nX_train = np.array(X_train)\ny_train = np.array(y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T21:32:42.572770Z","iopub.execute_input":"2025-03-08T21:32:42.573159Z","iopub.status.idle":"2025-03-08T21:32:42.601278Z","shell.execute_reply.started":"2025-03-08T21:32:42.573125Z","shell.execute_reply":"2025-03-08T21:32:42.599791Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"num_epochs = 10000\nbatch_size = 16\nlearning_rate = 0.0001\ninput_size = len(X_train[0])\nhidden_size = 16\noutput_size = len(tags)\nprint(input_size, output_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T21:32:42.603035Z","iopub.execute_input":"2025-03-08T21:32:42.603605Z","iopub.status.idle":"2025-03-08T21:32:42.626331Z","shell.execute_reply.started":"2025-03-08T21:32:42.603562Z","shell.execute_reply":"2025-03-08T21:32:42.625073Z"}},"outputs":[{"name":"stdout","text":"54 7\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"class ChatDataset(Dataset):\n    def __init__(self):\n        self.n_samples = len(X_train)\n        self.x_data = X_train\n        self.y_data = y_train\n        \n    def __getitem__(self, index):\n        return self.x_data[index], self.y_data[index]\n        \n    def __len__(self):\n        return self.n_samples","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T21:32:42.628984Z","iopub.execute_input":"2025-03-08T21:32:42.629343Z","iopub.status.idle":"2025-03-08T21:32:42.646696Z","shell.execute_reply.started":"2025-03-08T21:32:42.629309Z","shell.execute_reply":"2025-03-08T21:32:42.645545Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"dataset = ChatDataset()\ntrain_loader = DataLoader(dataset=dataset,\n                          batch_size=batch_size,\n                          shuffle=True,\n                          num_workers=0)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = NeuralNet(input_size, hidden_size, output_size).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T21:32:42.648348Z","iopub.execute_input":"2025-03-08T21:32:42.648682Z","iopub.status.idle":"2025-03-08T21:32:42.677606Z","shell.execute_reply.started":"2025-03-08T21:32:42.648654Z","shell.execute_reply":"2025-03-08T21:32:42.676336Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    for (words, labels) in train_loader:\n        words = words.to(device)\n        labels = labels.to(dtype=torch.long).to(device)\n\n        outputs = model(words)\n        # if y would be one-hot, we must apply\n        # labels = torch.max(labels, 1)[1]\n        loss = criterion(outputs, labels)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n    if (epoch+1) % 100 == 0:\n        print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\nprint(f'final loss: {loss.item():.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T21:32:42.678976Z","iopub.execute_input":"2025-03-08T21:32:42.679414Z","iopub.status.idle":"2025-03-08T21:33:16.698021Z","shell.execute_reply.started":"2025-03-08T21:32:42.679374Z","shell.execute_reply":"2025-03-08T21:33:16.696874Z"}},"outputs":[{"name":"stdout","text":"Epoch [100/10000], Loss: 1.9383\nEpoch [200/10000], Loss: 1.8873\nEpoch [300/10000], Loss: 1.8040\nEpoch [400/10000], Loss: 1.7746\nEpoch [500/10000], Loss: 1.6191\nEpoch [600/10000], Loss: 1.2795\nEpoch [700/10000], Loss: 1.1192\nEpoch [800/10000], Loss: 1.0292\nEpoch [900/10000], Loss: 0.9743\nEpoch [1000/10000], Loss: 0.5887\nEpoch [1100/10000], Loss: 0.6660\nEpoch [1200/10000], Loss: 0.5401\nEpoch [1300/10000], Loss: 0.4861\nEpoch [1400/10000], Loss: 0.3557\nEpoch [1500/10000], Loss: 0.2664\nEpoch [1600/10000], Loss: 0.3511\nEpoch [1700/10000], Loss: 0.2104\nEpoch [1800/10000], Loss: 0.2193\nEpoch [1900/10000], Loss: 0.2522\nEpoch [2000/10000], Loss: 0.1375\nEpoch [2100/10000], Loss: 0.0413\nEpoch [2200/10000], Loss: 0.1280\nEpoch [2300/10000], Loss: 0.0554\nEpoch [2400/10000], Loss: 0.0507\nEpoch [2500/10000], Loss: 0.0433\nEpoch [2600/10000], Loss: 0.0220\nEpoch [2700/10000], Loss: 0.0302\nEpoch [2800/10000], Loss: 0.0121\nEpoch [2900/10000], Loss: 0.0212\nEpoch [3000/10000], Loss: 0.0128\nEpoch [3100/10000], Loss: 0.0095\nEpoch [3200/10000], Loss: 0.0140\nEpoch [3300/10000], Loss: 0.0113\nEpoch [3400/10000], Loss: 0.0092\nEpoch [3500/10000], Loss: 0.0042\nEpoch [3600/10000], Loss: 0.0081\nEpoch [3700/10000], Loss: 0.0042\nEpoch [3800/10000], Loss: 0.0023\nEpoch [3900/10000], Loss: 0.0025\nEpoch [4000/10000], Loss: 0.0023\nEpoch [4100/10000], Loss: 0.0032\nEpoch [4200/10000], Loss: 0.0012\nEpoch [4300/10000], Loss: 0.0029\nEpoch [4400/10000], Loss: 0.0026\nEpoch [4500/10000], Loss: 0.0016\nEpoch [4600/10000], Loss: 0.0013\nEpoch [4700/10000], Loss: 0.0011\nEpoch [4800/10000], Loss: 0.0010\nEpoch [4900/10000], Loss: 0.0009\nEpoch [5000/10000], Loss: 0.0005\nEpoch [5100/10000], Loss: 0.0007\nEpoch [5200/10000], Loss: 0.0004\nEpoch [5300/10000], Loss: 0.0008\nEpoch [5400/10000], Loss: 0.0006\nEpoch [5500/10000], Loss: 0.0004\nEpoch [5600/10000], Loss: 0.0003\nEpoch [5700/10000], Loss: 0.0002\nEpoch [5800/10000], Loss: 0.0001\nEpoch [5900/10000], Loss: 0.0002\nEpoch [6000/10000], Loss: 0.0002\nEpoch [6100/10000], Loss: 0.0002\nEpoch [6200/10000], Loss: 0.0002\nEpoch [6300/10000], Loss: 0.0001\nEpoch [6400/10000], Loss: 0.0002\nEpoch [6500/10000], Loss: 0.0002\nEpoch [6600/10000], Loss: 0.0002\nEpoch [6700/10000], Loss: 0.0001\nEpoch [6800/10000], Loss: 0.0001\nEpoch [6900/10000], Loss: 0.0001\nEpoch [7000/10000], Loss: 0.0001\nEpoch [7100/10000], Loss: 0.0001\nEpoch [7200/10000], Loss: 0.0001\nEpoch [7300/10000], Loss: 0.0000\nEpoch [7400/10000], Loss: 0.0000\nEpoch [7500/10000], Loss: 0.0000\nEpoch [7600/10000], Loss: 0.0000\nEpoch [7700/10000], Loss: 0.0000\nEpoch [7800/10000], Loss: 0.0000\nEpoch [7900/10000], Loss: 0.0000\nEpoch [8000/10000], Loss: 0.0000\nEpoch [8100/10000], Loss: 0.0000\nEpoch [8200/10000], Loss: 0.0000\nEpoch [8300/10000], Loss: 0.0000\nEpoch [8400/10000], Loss: 0.0000\nEpoch [8500/10000], Loss: 0.0000\nEpoch [8600/10000], Loss: 0.0000\nEpoch [8700/10000], Loss: 0.0000\nEpoch [8800/10000], Loss: 0.0000\nEpoch [8900/10000], Loss: 0.0000\nEpoch [9000/10000], Loss: 0.0000\nEpoch [9100/10000], Loss: 0.0000\nEpoch [9200/10000], Loss: 0.0000\nEpoch [9300/10000], Loss: 0.0000\nEpoch [9400/10000], Loss: 0.0000\nEpoch [9500/10000], Loss: 0.0000\nEpoch [9600/10000], Loss: 0.0000\nEpoch [9700/10000], Loss: 0.0000\nEpoch [9800/10000], Loss: 0.0000\nEpoch [9900/10000], Loss: 0.0000\nEpoch [10000/10000], Loss: 0.0000\nfinal loss: 0.0000\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"data = {\n\"model_state\": model.state_dict(),\n\"input_size\": input_size,\n\"hidden_size\": hidden_size,\n\"output_size\": output_size,\n\"all_words\": all_words,\n\"tags\": tags\n}\n\nFILE = \"data.pth\"\ntorch.save(data, FILE)\n\nprint(f'training complete. file saved to {FILE}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T21:33:16.699161Z","iopub.execute_input":"2025-03-08T21:33:16.699884Z","iopub.status.idle":"2025-03-08T21:33:16.709124Z","shell.execute_reply.started":"2025-03-08T21:33:16.699851Z","shell.execute_reply":"2025-03-08T21:33:16.707997Z"}},"outputs":[{"name":"stdout","text":"training complete. file saved to data.pth\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"import random\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nFILE = \"/kaggle/working/data.pth\"\ndata = torch.load(FILE)\n\ninput_size = data[\"input_size\"]\nhidden_size = data[\"hidden_size\"]\noutput_size = data[\"output_size\"]\nall_words = data['all_words']\ntags = data['tags']\nmodel_state = data[\"model_state\"]\n\nmodel = NeuralNet(input_size, hidden_size, output_size).to(device)\nmodel.load_state_dict(model_state)\nmodel.eval()\n\nbot_name = \"Sam\"\nprint(\"Let's chat! (type 'quit' to exit)\")\nwhile True:\n    sentence = input(\"You: \")\n    if sentence == \"quit\":\n        break\n\n    sentence = tokenize(sentence)\n    X = bag_of_words(sentence, all_words)\n    X = X.reshape(1, X.shape[0])\n    X = torch.from_numpy(X).to(device)\n\n    output = model(X)\n    _, predicted = torch.max(output, dim=1)\n\n    tag = tags[predicted.item()]\n\n    probs = torch.softmax(output, dim=1)\n    prob = probs[0][predicted.item()]\n    if prob.item() > 0.75:\n        for intent in intents['intents']:\n            if tag == intent[\"tag\"]:\n                print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n    else:\n        print(f\"{bot_name}: I do not understand...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T21:33:16.710288Z","iopub.execute_input":"2025-03-08T21:33:16.710614Z","iopub.status.idle":"2025-03-08T21:37:24.579505Z","shell.execute_reply.started":"2025-03-08T21:33:16.710573Z","shell.execute_reply":"2025-03-08T21:37:24.578020Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-33-3856df23a16a>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  data = torch.load(FILE)\n","output_type":"stream"},{"name":"stdout","text":"Let's chat! (type 'quit' to exit)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  hey\n"},{"name":"stdout","text":"Sam: Hey :-)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  Hey\n"},{"name":"stdout","text":"Sam: Hi there, what can I do for you?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  hi\n"},{"name":"stdout","text":"Sam: Hi there, how can I help?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  Have a nice day\n"},{"name":"stdout","text":"Sam: Hi there, what can I do for you?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  thanks\n"},{"name":"stdout","text":"Sam: My pleasure\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  What do you sell\n"},{"name":"stdout","text":"Sam: We sell coffee and tea\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  Do you accept Mastercard?\n"},{"name":"stdout","text":"Sam: We accept most major credit cards, and Paypal\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  How long does delivery take\n"},{"name":"stdout","text":"Sam: Delivery takes 2-4 days\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  bye\n"},{"name":"stdout","text":"Sam: Bye! Come back again soon.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  quit\n"}],"execution_count":33},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}