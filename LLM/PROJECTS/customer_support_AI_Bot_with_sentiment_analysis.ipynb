{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMgB0oA0UxEa"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain langchain_core langchain_groq langchain_community langgraph gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Dict\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables.graph import MermaidDrawMethod\n",
        "from IPython.display import display , Image\n",
        "from langchain_groq import ChatGroq"
      ],
      "metadata": {
        "id": "E_XT640EYbK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class State(TypedDict):\n",
        "  query: str\n",
        "  catagory: str\n",
        "  sentiment: str\n",
        "  response: str"
      ],
      "metadata": {
        "id": "Kan9QyUdYbHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm= ChatGorq(\n",
        "    temperature= 0,\n",
        "    gorq_api_key= \"\",\n",
        "    model= \"\"\n",
        ")"
      ],
      "metadata": {
        "id": "87_zZmCWYbE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def categorize(state: State) -> State:\n",
        "  \"Technical, Billing, General\"\n",
        "  prompt = ChatPromptTemplate.from_template(\n",
        "      \"Categorize the following customer query into one of these categories: \"\n",
        "      \"Technical, Billing, General. Query: {query}\"\n",
        "  )\n",
        "  chain = prompt | llm\n",
        "  category = chain.invoke({\"query\": state[\"query\"]}).content\n",
        "  return {\"category\": category}\n",
        "\n",
        "def analyze_sentiment(state: State) -> State:\n",
        "  prompt = ChatPromptTemplate.from_template(\n",
        "      \"Analyze the sentiment of the following customer query\"\n",
        "      \"Response with either 'Position', 'Neutral' , or 'Negative'. Query: {query}\"\n",
        "  )\n",
        "  chain = prompt | llm\n",
        "  sentiment = chain.invoke({\"query\": state[\"query\"]}).content\n",
        "  return {\"sentiment\": sentiment}\n",
        "\n",
        "def handle_technical(state: State)->State:\n",
        "  prompt = ChatPromptTemplate.from_template(\n",
        "      \"Provide a technical support response to the following query : {query}\"\n",
        "  )\n",
        "  chain = prompt | llm\n",
        "  response = chain.invoke({\"query\": state[\"query\"]}).content\n",
        "  return {\"response\": response}\n",
        "\n",
        "def handle_billing(state: State)->State:\n",
        "  prompt = ChatPromptTemplate.from_template(\n",
        "      \"Provide a billing support response to the following query : {query}\"\n",
        "  )\n",
        "  chain = prompt | llm\n",
        "  response = chain.invoke({\"query\": state[\"query\"]}).content\n",
        "  return {\"response\": response}\n",
        "\n",
        "def handle_general(state: State)->State:\n",
        "  prompt = ChatPromptTemplate.from_template(\n",
        "      \"Provide a general support response to the following query : {query}\"\n",
        "  )\n",
        "  chain = prompt | llm\n",
        "  response = chain.invoke({\"query\": state[\"query\"]}).content\n",
        "  return {\"response\": response}\n",
        "\n",
        "def escalate(state: State)->State:\n",
        "  return {\"response\": \"This query has been escalate to a human agent due to its negative sentiment\"}\n",
        "\n",
        "def route_query(state: State)->State:\n",
        "  if state[\"sentiment\"] == \"Negative\":\n",
        "    return \"escalate\"\n",
        "  elif state[\"category\"] == \"Technical\":\n",
        "    return \"handle_technical\"\n",
        "  elif state[\"category\"] == \"Billing\":\n",
        "    return \"handle_billing\"\n",
        "  else:\n",
        "    return \"handle_general\""
      ],
      "metadata": {
        "id": "YgcoVKJOYbCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workflow = StateGraph(State)\n",
        "\n",
        "workflow.add_node(\"categorize\", categorize)\n",
        "workflow.add_node(\"analyze_sentiment\", analyze_sentiment)\n",
        "workflow.add_node(\"handle_technical\", handle_technical)\n",
        "workflow.add_node(\"handle_billing\", handle_billing)\n",
        "workflow.add_node(\"handle_general\", handle_general)\n",
        "workflow.add_node(\"escalate\", escalate)\n",
        "\n",
        "workflow.add_edge(\"categorize\", \"analyze_sentiment\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"analyze_sentiment\",\n",
        "    route_query,{\n",
        "        \"handle_technical\" : \"handle_technical\",\n",
        "        \"handle_billing\" :  \"handle_billing\",\n",
        "        \"handle_general\" : \"handle_general\",\n",
        "        \"escalate\": \"escalate\"\n",
        "    }\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"handle_technical\", END)\n",
        "workflow.add_edge(\"handle_billing\", END)\n",
        "workflow.add_edge(\"handle_general\", END)\n",
        "workflow.add_edge(\"escalate\", END)\n",
        "\n",
        "workflow.set_entry_point(\"categorize\")\n",
        "app  = workflow.compile()\n",
        "\n",
        "\n",
        "display(\n",
        "    Image(\n",
        "        app.get_graph().draw_mermaid_png(\n",
        "            draw_method=MermaidDrawMethod.API\n",
        "        )\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "Wp1u5la5Ya_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main bot"
      ],
      "metadata": {
        "id": "Vq36z-hh1wpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Dict\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables.graph import MermaidDrawMethod\n",
        "from IPython.display import display, Image\n",
        "\n",
        "class State(TypedDict):\n",
        "    query: str\n",
        "    category: str\n",
        "    sentiment: str\n",
        "    response: str\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(\n",
        "    temperature=0,\n",
        "    groq_api_key=\"Your_api\",\n",
        "    model_name=\"llama-3.3-70b-versatile\"\n",
        ")\n",
        "\n",
        "\n",
        "def categorize(state: State) -> State:\n",
        "    \"\"\"Categorize the query.\"\"\"\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"Categorize the following customer query into one of these categories: \"\n",
        "        \"Technical, Billing, General. Query: {query}\"\n",
        "    )\n",
        "    chain = prompt | llm\n",
        "    category = chain.invoke({\"query\": state[\"query\"]}).content\n",
        "    return {\"category\": category}\n",
        "\n",
        "def analyze_sentiment(state: State) -> State:\n",
        "    \"\"\"Analyze sentiment of the query.\"\"\"\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"Analyze the sentiment of the following customer query \"\n",
        "        \"Response with either 'Positive', 'Neutral', or 'Negative'. Query: {query}\"\n",
        "    )\n",
        "    chain = prompt | llm\n",
        "    sentiment = chain.invoke({\"query\": state[\"query\"]}).content\n",
        "    return {\"sentiment\": sentiment}\n",
        "\n",
        "def handle_technical(state: State) -> State:\n",
        "    \"\"\"Handle technical queries.\"\"\"\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"Provide a technical support response to the following query: {query}\"\n",
        "    )\n",
        "    chain = prompt | llm\n",
        "    response = chain.invoke({\"query\": state[\"query\"]}).content\n",
        "    return {\"response\": response}\n",
        "\n",
        "def handle_billing(state: State) -> State:\n",
        "    \"\"\"Handle billing queries.\"\"\"\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"Provide a billing support response to the following query: {query}\"\n",
        "    )\n",
        "    chain = prompt | llm\n",
        "    response = chain.invoke({\"query\": state[\"query\"]}).content\n",
        "    return {\"response\": response}\n",
        "\n",
        "def handle_general(state: State) -> State:\n",
        "    \"\"\"Handle general queries.\"\"\"\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"Provide a general support response to the following query: {query}\"\n",
        "    )\n",
        "    chain = prompt | llm\n",
        "    response = chain.invoke({\"query\": state[\"query\"]}).content\n",
        "    return {\"response\": response}\n",
        "\n",
        "def escalate(state: State) -> State:\n",
        "    \"\"\"Escalate negative sentiment queries.\"\"\"\n",
        "    return {\"response\": \"This query has been escalated to a human agent due to its negative sentiment.\"}\n",
        "\n",
        "def route_query(state: State) -> State:\n",
        "    \"\"\"Route query based on category and sentiment.\"\"\"\n",
        "    if state[\"sentiment\"] == \"Negative\":\n",
        "        return \"escalate\"\n",
        "    elif state[\"category\"] == \"Technical\":\n",
        "        return \"handle_technical\"\n",
        "    elif state[\"category\"] == \"Billing\":\n",
        "        return \"handle_billing\"\n",
        "    else:\n",
        "        return \"handle_general\"\n",
        "\n",
        "\n",
        "\n",
        "import gradio as gr\n",
        "chat_history = []\n",
        "\n",
        "def run_customer_support(query: str) -> Dict[str, str]:\n",
        "    results = app.invoke({\"query\": query})\n",
        "    entry = {\n",
        "        \"query\": query,\n",
        "        \"category\": results['category'],\n",
        "        \"sentiment\": results['sentiment'],\n",
        "        \"response\": results['response']\n",
        "    }\n",
        "    chat_history.append(entry)\n",
        "    return entry\n",
        "\n",
        "def gradio_interface(query: str):\n",
        "    result = run_customer_support(query)\n",
        "    history_md = \"\"\n",
        "    for i, entry in enumerate(chat_history, 1):\n",
        "        history_md += (\n",
        "            f\"### üßë‚Äçüí¨ Query {i}:\\n\"\n",
        "            f\"**You:** {entry['query']}\\n\\n\"\n",
        "            f\"**Category:** {entry['category']}  \\n\"\n",
        "            f\"**Sentiment:** {entry['sentiment']}  \\n\"\n",
        "            f\"**Bot:** {entry['response']}\\n\\n\"\n",
        "            \"---\\n\"\n",
        "        )\n",
        "    return history_md\n",
        "gui = gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    theme='NoCrypt/miku',\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your query here...\"),\n",
        "    outputs=gr.Markdown(),\n",
        "    title=\"Customer Support Assistant with Sentiment Analysis\",\n",
        "    description=\"Ask your question and get instant, smart support ‚Äî categorized, sentiment-aware, and handled just right!\",\n",
        ")\n",
        "if __name__ == \"__main__\":\n",
        "    gui.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "q_YC6Yh21yOu",
        "outputId": "95530a1c-ce91-4607-e3f7-05bb6f5cc855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://45aa073fb55af0c42a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://45aa073fb55af0c42a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CLNmin80Yavt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ejlS1SNMYatG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f3CBfqSBYaqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sv1OFhWsYand"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "84-4Mv71Yakt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nct55P7lYaem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P3K-hCi4Yace"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4nLqlBlMYaZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WUqUzDyQYaW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Endmq_KCYaT2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}