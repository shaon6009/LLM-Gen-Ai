{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CausalAttention","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass CausalAttention(nn.Module):\n\n    def __init__(self, d_in, d_out, context_length,\n                 dropout, qkv_bias=False):\n        super().__init__()\n        self.d_out = d_out\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.dropout = nn.Dropout(dropout) # New\n        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape # New batch dimension b\n        keys = self.W_key(x)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n        attn_scores.masked_fill_(  # New, _ ops are in-place\n            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n        attn_weights = torch.softmax(\n            attn_scores / keys.shape[-1]**0.5, dim=-1\n        )\n        attn_weights = self.dropout(attn_weights) # New\n\n        context_vec = attn_weights @ values\n        return context_vec","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-28T16:58:39.778758Z","iopub.execute_input":"2024-11-28T16:58:39.779628Z","iopub.status.idle":"2024-11-28T16:58:42.789670Z","shell.execute_reply.started":"2024-11-28T16:58:39.779585Z","shell.execute_reply":"2024-11-28T16:58:42.788671Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"batch_size = 4\nnum_tokens = 10  \nd_in = 16\nd_out = 3\nbatch = torch.randn(batch_size, num_tokens, d_in)\n\ntorch.manual_seed(123)\ncontext_length = batch.shape[1]\nca = CausalAttention(d_in, d_out, context_length, 0.0)\ncontext_vecs = ca(batch)\nprint(\"context_vecs.shape:\", context_vecs.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T16:59:45.953072Z","iopub.execute_input":"2024-11-28T16:59:45.953458Z","iopub.status.idle":"2024-11-28T16:59:46.092367Z","shell.execute_reply.started":"2024-11-28T16:59:45.953426Z","shell.execute_reply":"2024-11-28T16:59:46.091283Z"}},"outputs":[{"name":"stdout","text":"context_vecs.shape: torch.Size([4, 10, 3])\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"Query, Key, Value (QKV) তৈরি করা:\nপ্রতিটি ইনপুট টোকেনের জন্য Query (Q), Key (K), এবং Value (V) তৈরি করা হয়।\nQuery (Q): বর্তমান টোকেন কী খুঁজছে।\nKey (K): অন্য টোকেনগুলোর প্রাসঙ্গিকতা (relevance)।\nValue (V): সেই প্রাসঙ্গিক টোকেন থেকে প্রাপ্ত তথ্য।\nকেন করি?\nকারণ attention মেকানিজমের মাধ্যমে মডেলকে শেখাতে চাই কোন টোকেনগুলো অন্য টোকেনের সাথে কতটা সম্পর্কিত।\nDimension বিভাজন করা (Multi-Head):\nইনপুট ডেটার dimension num_heads দিয়ে ভাগ করে প্রতিটি head-এ আলাদা ফিচার শেখার সুযোগ তৈরি করা হয়।\n\nকেন করি?\nএকাধিক head ব্যবহার করলে ইনপুট ডেটার বিভিন্ন প্যাটার্ন বা সম্পর্ক বুঝতে মডেল আরও ভালো হয়। উদাহরণস্বরূপ, একটি head subject-object সম্পর্ক শেখে, আরেকটি head tense বা সময় বোঝে।\n\nAttention Scores গণনা:\n               T\nScore=Query⋅Key \n\n\nএটি মূলত একটি ডট প্রোডাক্ট যা বলে দেয়, বর্তমান টোকেন (Query) অন্যান্য টোকেন (Key)-এর সাথে কতটা সম্পর্কযুক্ত।\nকেন করি?\nএটি নিশ্চিত করে যে কোন টোকেনগুলোর উপর বেশি ফোকাস করতে হবে। স্কোর যত বেশি, ফোকাস তত বেশি।\n\nScaled এবং Softmax প্রয়োগ করা:\n\nScaled করার জন্য \nScore/(route)dimensionকরা হয়।\n​\nSoftmax দিয়ে স্কোরগুলোকে probability distribution-এ রূপান্তর করা হয়।\nকেন করি?\n\nScaled করার মাধ্যমে অত্যধিক বড় বা ছোট স্কোর থেকে numerical instability রোধ হয়।\nSoftmax প্রয়োগের মাধ্যমে নিশ্চিত করা হয় যে টোকেনগুলোর মধ্যে প্রায়োরিটি তৈরি হয়।\nMask প্রয়োগ করা:\n\nভবিষ্যতের টোকেনগুলোর উপর ফোকাস রোধ করতে একটি mask ব্যবহার করা হয়।\nMasked এলিমেন্টগুলোর স্কোর -inf দিয়ে পূরণ করা হয়, যাতে সেগুলোর softmax মান শূন্য হয়।\nকেন করি?\nকারণ অনেক অ্যাপ্লিকেশন (যেমন: ভাষা মডেল) ভবিষ্যতের টোকেন দেখে ফলাফল তৈরি করতে পারে না। এটি শুধুমাত্র বর্তমান এবং অতীত টোকেনের ওপর নির্ভর করে।\n\nWeighted Sum (Attention Output):\nContext\n=\nAttention Weights\n⋅\nValue\nContext=Attention Weights⋅Value\n\nAttention weights ব্যবহার করে Value ভেক্টরের weighted sum নেওয়া হয়।\nকেন করি?\nএটি প্রতিটি টোকেনের জন্য কনটেক্সট তৈরি করে যা বলে দেয়, সিকোয়েন্সের অন্য টোকেনগুলো কতটা গুরুত্বপূর্ণ।\n\nHead Outputs Concatenate করা:\nপ্রতিটি head থেকে পাওয়া আউটপুট একত্রিত করে একটি ফাইনাল ভেক্টর তৈরি করা হয়।\n\nকেন করি?\nএটি নিশ্চিত করে যে মডেল ইনপুট ডেটার বিভিন্ন দৃষ্টিকোণ (perspective) থেকে সম্পর্ক বোঝে।\n\nFinal Linear Projection:\nআউটপুট ভেক্টরকে একটি লিনিয়ার লেয়ার দিয়ে প্রজেক্ট করা হয় যাতে চূড়ান্ত আকার (dimension) পাওয়া যায়।\n\nকেন করি?\nমডেলের জন্য আউটপুট সঠিক আকারে (dimension) নিয়ে আসা প্রয়োজন যাতে এটি পরবর্তী ধাপে ব্যবহারযোগ্য হয়।","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        assert (d_out % num_heads == 0), \\\n            \"d_out must be divisible by num_heads\"\n\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer(\n            \"mask\",\n            torch.triu(torch.ones(context_length, context_length),\n                       diagonal=1)\n        )\n\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape\n\n        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        # We implicitly split the matrix by adding a `num_heads` dimension\n        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n\n        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n        keys = keys.transpose(1, 2)\n        queries = queries.transpose(1, 2)\n        values = values.transpose(1, 2)\n\n        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n\n        # Original mask truncated to the number of tokens and converted to boolean\n        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n\n        # Use the mask to fill attention scores\n        attn_scores.masked_fill_(mask_bool, -torch.inf)\n        \n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        # Shape: (b, num_tokens, num_heads, head_dim)\n        context_vec = (attn_weights @ values).transpose(1, 2) \n        \n        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n        context_vec = self.out_proj(context_vec) # optional projection\n\n        return context_vec","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T17:00:36.558634Z","iopub.execute_input":"2024-11-28T17:00:36.559848Z","iopub.status.idle":"2024-11-28T17:00:36.570830Z","shell.execute_reply.started":"2024-11-28T17:00:36.559799Z","shell.execute_reply":"2024-11-28T17:00:36.569823Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573], #A\n[0.8993, 0.0390, 0.9268, 0.7388],\n[0.7179, 0.7058, 0.9156, 0.4340]],\n[[0.0772, 0.3565, 0.1479, 0.5331],\n[0.4066, 0.2318, 0.4545, 0.9737],\n[0.4606, 0.5159, 0.4220, 0.5786]]]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T17:00:48.625997Z","iopub.execute_input":"2024-11-28T17:00:48.626371Z","iopub.status.idle":"2024-11-28T17:00:48.632469Z","shell.execute_reply.started":"2024-11-28T17:00:48.626340Z","shell.execute_reply":"2024-11-28T17:00:48.631167Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"print(a @ a.transpose(2, 3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T17:00:57.807061Z","iopub.execute_input":"2024-11-28T17:00:57.807430Z","iopub.status.idle":"2024-11-28T17:00:57.844197Z","shell.execute_reply.started":"2024-11-28T17:00:57.807398Z","shell.execute_reply":"2024-11-28T17:00:57.842863Z"}},"outputs":[{"name":"stdout","text":"tensor([[[[1.3208, 1.1631, 1.2879],\n          [1.1631, 2.2150, 1.8424],\n          [1.2879, 1.8424, 2.0402]],\n\n         [[0.4391, 0.7003, 0.5903],\n          [0.7003, 1.3737, 1.0620],\n          [0.5903, 1.0620, 0.9912]]]])\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"first_head = a[0, 0, :, :]\nfirst_res = first_head @ first_head.T\nprint(\"First head:\\n\", first_res)\nsecond_head = a[0, 1, :, :]\nsecond_res = second_head @ second_head.T\nprint(\"\\nSecond head:\\n\", second_res)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T17:01:11.024635Z","iopub.execute_input":"2024-11-28T17:01:11.025025Z","iopub.status.idle":"2024-11-28T17:01:11.037378Z","shell.execute_reply.started":"2024-11-28T17:01:11.024988Z","shell.execute_reply":"2024-11-28T17:01:11.036409Z"}},"outputs":[{"name":"stdout","text":"First head:\n tensor([[1.3208, 1.1631, 1.2879],\n        [1.1631, 2.2150, 1.8424],\n        [1.2879, 1.8424, 2.0402]])\n\nSecond head:\n tensor([[0.4391, 0.7003, 0.5903],\n        [0.7003, 1.3737, 1.0620],\n        [0.5903, 1.0620, 0.9912]])\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"torch.manual_seed(123)\nbatch_size, context_length, d_in = batch.shape\nd_out = 2\nmha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\ncontext_vecs = mha(batch)\nprint(context_vecs)\nprint(\"context_vecs.shape:\", context_vecs.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T17:01:21.274300Z","iopub.execute_input":"2024-11-28T17:01:21.274780Z","iopub.status.idle":"2024-11-28T17:01:21.293182Z","shell.execute_reply.started":"2024-11-28T17:01:21.274740Z","shell.execute_reply":"2024-11-28T17:01:21.292013Z"}},"outputs":[{"name":"stdout","text":"tensor([[[ 0.1937, -0.7206],\n         [-0.0246, -0.6600],\n         [-0.0181, -0.7055],\n         [ 0.2677, -0.5520],\n         [ 0.2688, -0.6845],\n         [ 0.3168, -0.6202],\n         [ 0.2888, -0.6215],\n         [ 0.2699, -0.7250],\n         [ 0.3662, -0.6551],\n         [ 0.2967, -0.6294]],\n\n        [[ 1.9564,  0.4715],\n         [ 0.8358, -0.1561],\n         [ 0.8107, -0.2891],\n         [ 0.6754, -0.3448],\n         [ 0.5698, -0.5668],\n         [ 0.5682, -0.5072],\n         [ 0.5189, -0.5563],\n         [ 0.6367, -0.4840],\n         [ 0.6981, -0.4700],\n         [ 0.4923, -0.6446]],\n\n        [[ 0.6105, -0.8314],\n         [ 0.5328, -0.4183],\n         [ 0.6882, -0.3075],\n         [ 0.5568, -0.2954],\n         [ 0.5763, -0.2767],\n         [ 0.6404, -0.2021],\n         [ 0.6499, -0.2396],\n         [ 0.6597, -0.2663],\n         [ 0.6579, -0.3026],\n         [ 0.6101, -0.3756]],\n\n        [[ 1.0248, -0.2336],\n         [ 0.5855, -0.5365],\n         [ 0.6044, -0.3609],\n         [ 0.3708, -0.5311],\n         [ 0.4972, -0.4520],\n         [ 0.4092, -0.4409],\n         [ 0.3571, -0.5686],\n         [ 0.3095, -0.5724],\n         [ 0.3387, -0.5416],\n         [ 0.3921, -0.4889]]], grad_fn=<ViewBackward0>)\ncontext_vecs.shape: torch.Size([4, 10, 2])\n","output_type":"stream"}],"execution_count":10}]}